import torch
import sounddevice as sd
import numpy as np
import os
import warnings
from scipy.io import wavfile
from scipy.signal import resample
from pathlib import Path
import sys
from dotenv import load_dotenv
from speechbrain.pretrained import EncoderClassifier
# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore")
os.environ['TORCHAUDIO_USE_SOX_EFFECTS'] = '0'
load_dotenv()

# ====== CRITICAL: speechbrain import ì „ì— ë°±ì—”ë“œ ì²´í¬ ì™„ì „ ì°¨ë‹¨ ======
import torchaudio
import torchaudio.backend.utils
import torchaudio.utils

# ë°±ì—”ë“œ ì²´í¬ í•¨ìˆ˜ ë¬´ì‹œ
def noop(*args, **kwargs):
    pass

amplifier = int(os.getenv("voice_amplifier"))

torchaudio.set_audio_backend = noop
torchaudio.backend.utils.set_audio_backend = noop
if hasattr(torchaudio, 'utils'):
    if hasattr(torchaudio.utils, 'check_torchaudio_backend'):
        torchaudio.utils.check_torchaudio_backend = noop

# ====== ì´ì œ speechbrain import ======

# Windowsì—ì„œ symlink ì—ëŸ¬ í•´ê²°ì„ ìœ„í•´ pathlib íŒ¨ì¹˜
original_symlink_to = Path.symlink_to
def patched_symlink_to(self, target, target_is_directory=False):
    """symlink ëŒ€ì‹  íŒŒì¼ ë³µì‚¬ ì‚¬ìš©"""
    import shutil
    target = Path(target)
    if target.is_file():
        self.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(target, self)
    elif target.is_dir():
        self.parent.mkdir(parents=True, exist_ok=True)
        if self.exists():
            shutil.rmtree(self)
        shutil.copytree(target, self)

Path.symlink_to = patched_symlink_to

SAMPLE_RATE = 16000
DURATION = 4  # seconds per recognition chunk

print("ëª¨ë¸ ë¡œë”© ì¤‘...")
try:
    
    classifier = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir="pretrained_models/ecapa"
    )
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()
    raise

# ì›ë³¸ í•¨ìˆ˜ ë³µì›
Path.symlink_to = original_symlink_to

import queue
audio_queue = queue.Queue()

def audio_callback(indata, frames, time, status):
    """ì˜¤ë””ì˜¤ ì½œë°± í•¨ìˆ˜"""
    audio_queue.put(indata.copy())

def load_audio_file(file_path):
    """scipyë¥¼ ì‚¬ìš©í•˜ì—¬ wav íŒŒì¼ ë¡œë“œ"""
    sr, audio = wavfile.read(file_path)
    
    # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
    if len(audio.shape) > 1:
        audio = audio[:, 0]
    
    # float32ë¡œ ë³€í™˜
    audio = audio.astype(np.float32)
    
    # ì •ê·œí™” (int16 ë²”ìœ„ ê³ ë ¤)
    if audio.dtype == np.int16:
        audio = audio / 32768.0
    
    # ìƒ˜í”Œë ˆì´íŠ¸ ë¦¬ìƒ˜í”Œë§
    if sr != SAMPLE_RATE:
        num_samples = int(len(audio) * SAMPLE_RATE / sr)
        audio = resample(audio, num_samples)
    
    return torch.tensor(audio, dtype=torch.float32)

def record_audio_chunk():
    """ë§ˆì´í¬ì—ì„œ ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì§‘"""
    frames = []
    needed_frames = int(SAMPLE_RATE * DURATION)

    while len(frames) < needed_frames:
        data = audio_queue.get()
        frames.extend(data[:, 0])

    return np.array(frames[:needed_frames], dtype=np.float32)

def extract_embedding_from_array(audio_array):
    """ì˜¤ë””ì˜¤ ì„ë² ë”© ì¶”ì¶œ"""
    tensor_audio = torch.tensor(audio_array).unsqueeze(0)
    with torch.no_grad():
        embedding = classifier.encode_batch(tensor_audio)
    return embedding.squeeze(0)

def cosine_similarity(a, b):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìŠ¤ì¹¼ë¼ ê°’ ë°˜í™˜)"""
    # embeddingì´ 1D ë²¡í„°ì¸ ê²½ìš°
    if a.dim() == 1:
        a = a.unsqueeze(0)
    if b.dim() == 1:
        b = b.unsqueeze(0)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sim = torch.nn.functional.cosine_similarity(a, b)
    # ìŠ¤ì¹¼ë¼ ê°’ ë°˜í™˜
    return sim.mean() if sim.numel() > 1 else sim

print("ë“±ë¡ëœ í™”ì ìŒì„± ë¡œë”© ì¤‘...")

try:
    registered_users = {
        "chulsu": extract_embedding_from_array(
            load_audio_file("register/chulsu.wav")
        ),
        "younghee": extract_embedding_from_array(
            load_audio_file("register/younghee.wav")
        ),
        "dongin": extract_embedding_from_array(
            load_audio_file("register/dongin.wav")
        )
    }
    print("âœ“ í™”ì ë“±ë¡ ì™„ë£Œ")
except FileNotFoundError as e:
    print(f"âœ— ì˜¤ë¥˜: ë“±ë¡ëœ wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}")
    exit(1)
except Exception as e:
    print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

print("\nì‹¤ì‹œê°„ í™”ì ì¸ì‹ ì‹œì‘ (Ctrl+C ì¢…ë£Œ)")
print("-" * 50)

try:
    with sd.InputStream(callback=audio_callback,samplerate=SAMPLE_RATE,channels=1):

        while True:
            print("ìŒì„± ìˆ˜ì‹ ì¤‘... (4ì´ˆ ëŒ€ê¸°)")
            audio_chunk = record_audio_chunk()
            test_embedding = extract_embedding_from_array(audio_chunk)

            best_score = -1
            best_user = "Unknown"

            for name, emb in registered_users.items():
                score = cosine_similarity(test_embedding, emb)
                score_val = score.item() if torch.is_tensor(score) else float(score)
                if score_val > best_score:
                    best_score = score_val
                    best_user = name

            print(f"ğŸ‘¤ ì¸ì‹ëœ í™”ì: {best_user:10s} | ìœ ì‚¬ë„: {best_score * amplifier:.4f}")
            print("-" * 50)

except KeyboardInterrupt:
    print("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ")
except Exception as e:
    print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
