import cv2
import mediapipe as mp
import numpy as np
from datetime import datetime
import json

class MediaPipeDetector:
    """
    MediaPipe ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ + ì œìŠ¤ì²˜ ì¸ì‹
    - Face Detection
    - Hand Gesture Recognition (thumbs up, peace sign, etc.)
    """
    
    def __init__(self):
        # Face Detection ì´ˆê¸°í™”
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detector = self.mp_face_detection.FaceDetection(
            model_selection=1,  # 0: short range (2m), 1: full range (5m)
            min_detection_confidence=0.7
        )
        
        # Hand Detection ì´ˆê¸°í™”
        self.mp_hands = mp.solutions.hands
        self.hands_detector = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
        # Drawing utilities
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        print("âœ“ MediaPipe Face + Gesture Detector initialized")
    
    def detect_faces(self, frame):
        """
        ì–¼êµ´ ê²€ì¶œ
        
        Returns:
            faces: [{'bbox': [x, y, w, h], 'confidence': float, 'landmarks': {...}}, ...]
        """
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(rgb_frame)
        
        faces = []
        if results.detections:
            height, width = frame.shape[:2]
            
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box
                x = int(bbox.xmin * width)
                y = int(bbox.ymin * height)
                w = int(bbox.width * width)
                h = int(bbox.height * height)
                confidence = detection.score[0]
                
                # Landmarks (ëˆˆ, ì½”, ì… ë“±)
                landmarks = {}
                if detection.location_data.relative_keypoints:
                    for idx, keypoint in enumerate(detection.location_data.relative_keypoints):
                        landmarks[f'point_{idx}'] = {
                            'x': int(keypoint.x * width),
                            'y': int(keypoint.y * height)
                        }
                
                faces.append({
                    'bbox': [x, y, w, h],
                    'confidence': float(confidence),
                    'landmarks': landmarks
                })
        
        return faces
    
    def detect_gestures(self, frame):
        """
        ì† ì œìŠ¤ì²˜ ì¸ì‹
        
        Returns:
            gestures: [{'type': str, 'hand': 'left'|'right', 'confidence': float, 'landmarks': [...]}, ...]
        """
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands_detector.process(rgb_frame)
        
        gestures = []
        if results.multi_hand_landmarks and results.multi_handedness:
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                # ì†ì˜ ì™¼ìª½/ì˜¤ë¥¸ìª½ êµ¬ë¶„
                hand_label = handedness.classification[0].label  # 'Left' or 'Right'
                hand_confidence = handedness.classification[0].score
                
                # Landmarks ì¶”ì¶œ
                landmarks_list = []
                for landmark in hand_landmarks.landmark:
                    landmarks_list.append({
                        'x': landmark.x,
                        'y': landmark.y,
                        'z': landmark.z
                    })
                
                # ì œìŠ¤ì²˜ ì¸ì‹
                gesture_type = self._recognize_gesture(hand_landmarks)
                
                gestures.append({
                    'type': gesture_type,
                    'hand': hand_label.lower(),
                    'confidence': float(hand_confidence),
                    'landmarks': landmarks_list
                })
        
        return gestures
    
    def _recognize_gesture(self, hand_landmarks):
        """
        ì† ëœë“œë§ˆí¬ë¡œë¶€í„° ì œìŠ¤ì²˜ ì¸ì‹
        
        Gestures:
        - thumbs_up: ì—„ì§€ ì˜¬ë¦¼ (ì¶œì„ ì²´í¬)
        - peace: ë¸Œì´ ì‚¬ì¸ (í™•ì¸)
        - fist: ì£¼ë¨¹ (ì·¨ì†Œ)
        - open_palm: ì†ë°”ë‹¥ í¼ì¹¨ (ëŒ€ê¸°)
        - pointing: ê²€ì§€ ê°€ë¦¬í‚´
        """
        landmarks = hand_landmarks.landmark
        
        # ì†ê°€ë½ ëì ê³¼ ê´€ì ˆì 
        thumb_tip = landmarks[self.mp_hands.HandLandmark.THUMB_TIP]
        thumb_ip = landmarks[self.mp_hands.HandLandmark.THUMB_IP]
        
        index_tip = landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]
        
        middle_tip = landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
        middle_pip = landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP]
        
        ring_tip = landmarks[self.mp_hands.HandLandmark.RING_FINGER_TIP]
        ring_pip = landmarks[self.mp_hands.HandLandmark.RING_FINGER_PIP]
        
        pinky_tip = landmarks[self.mp_hands.HandLandmark.PINKY_TIP]
        pinky_pip = landmarks[self.mp_hands.HandLandmark.PINKY_PIP]
        
        wrist = landmarks[self.mp_hands.HandLandmark.WRIST]
        
        # ê° ì†ê°€ë½ì´ í´ì ¸ìˆëŠ”ì§€ í™•ì¸
        thumb_extended = thumb_tip.y < thumb_ip.y
        index_extended = index_tip.y < index_pip.y
        middle_extended = middle_tip.y < middle_pip.y
        ring_extended = ring_tip.y < ring_pip.y
        pinky_extended = pinky_tip.y < pinky_pip.y
        
        # ì œìŠ¤ì²˜ íŒë³„
        
        # Thumbs Up (ì—„ì§€ë§Œ í´ì§, ë‚˜ë¨¸ì§€ ì ‘í˜)
        if thumb_extended and not (index_extended or middle_extended or ring_extended or pinky_extended):
            if thumb_tip.y < wrist.y:  # ì—„ì§€ê°€ ìœ„ë¡œ
                return "thumbs_up"
        
        # Peace Sign (ê²€ì§€, ì¤‘ì§€ë§Œ í´ì§)
        if index_extended and middle_extended and not (ring_extended or pinky_extended):
            return "peace"
        
        # Fist (ëª¨ë‘ ì ‘í˜)
        if not (thumb_extended or index_extended or middle_extended or ring_extended or pinky_extended):
            return "fist"
        
        # Open Palm (ëª¨ë‘ í´ì§)
        if thumb_extended and index_extended and middle_extended and ring_extended and pinky_extended:
            return "open_palm"
        
        # Pointing (ê²€ì§€ë§Œ í´ì§)
        if index_extended and not (middle_extended or ring_extended or pinky_extended):
            return "pointing"
        
        return "unknown"
    
    def draw_detections(self, frame, faces, gestures):
        """
        ê²€ì¶œ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
        """
        annotated = frame.copy()
        
        # ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        for face in faces:
            x, y, w, h = face['bbox']
            confidence = face['confidence']
            
            # ë°•ìŠ¤
            color = (0, 255, 0) if confidence > 0.8 else (0, 165, 255)
            cv2.rectangle(annotated, (x, y), (x + w, y + h), color, 2)
            
            # ì‹ ë¢°ë„
            label = f"Face: {confidence:.2f}"
            cv2.putText(annotated, label, (x, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            # ëœë“œë§ˆí¬
            for point_name, point in face['landmarks'].items():
                cv2.circle(annotated, (point['x'], point['y']), 3, (255, 0, 0), -1)
        
        # ì œìŠ¤ì²˜ ì •ë³´ í‘œì‹œ
        y_offset = 30
        for i, gesture in enumerate(gestures):
            gesture_text = f"{gesture['hand'].upper()} Hand: {gesture['type']} ({gesture['confidence']:.2f})"
            
            # ì œìŠ¤ì²˜ë³„ ìƒ‰ìƒ
            color_map = {
                'thumbs_up': (0, 255, 0),
                'peace': (255, 255, 0),
                'fist': (0, 0, 255),
                'open_palm': (255, 255, 255),
                'pointing': (255, 128, 0)
            }
            color = color_map.get(gesture['type'], (128, 128, 128))
            
            cv2.putText(annotated, gesture_text, (10, y_offset + i * 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        return annotated
    
    def process_frame(self, frame):
        """
        í”„ë ˆì„ ì²˜ë¦¬ - ì–¼êµ´ + ì œìŠ¤ì²˜ í†µí•©
        
        Returns:
            {
                'faces': [...],
                'gestures': [...],
                'timestamp': float,
                'frame_annotated': numpy.ndarray
            }
        """
        faces = self.detect_faces(frame)
        gestures = self.detect_gestures(frame)
        annotated = self.draw_detections(frame, faces, gestures)
        
        return {
            'faces': faces,
            'gestures': gestures,
            'timestamp': datetime.now().timestamp(),
            'frame_annotated': annotated
        }
    
    def to_json(self, result):
        """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë³€í™˜ (ZeroMQ ì „ì†¡ìš©)"""
        return json.dumps({
            'faces': result['faces'],
            'gestures': [{
                'type': g['type'],
                'hand': g['hand'],
                'confidence': g['confidence']
            } for g in result['gestures']],
            'timestamp': result['timestamp']
        })
    
    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        self.face_detector.close()
        self.hands_detector.close()


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    detector = MediaPipeDetector()
    cap = cv2.VideoCapture(0)
    
    print("\n=== ì œìŠ¤ì²˜ ê°€ì´ë“œ ===")
    print("ğŸ‘ Thumbs Up: ì¶œì„ ì²´í¬")
    print("âœŒï¸  Peace Sign: í™•ì¸")
    print("âœŠ Fist: ì·¨ì†Œ")
    print("âœ‹ Open Palm: ëŒ€ê¸°")
    print("â˜ï¸  Pointing: ì„ íƒ")
    print("\nPress 'q' to quit, 's' to save screenshot")
    
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ë§¤ í”„ë ˆì„ ì²˜ë¦¬ (ì„±ëŠ¥ì„ ìœ„í•´ ë§¤ 3í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬ ê°€ëŠ¥)
        if frame_count % 1 == 0:
            result = detector.process_frame(frame)
            
            # ì •ë³´ í‘œì‹œ
            info_text = f"Faces: {len(result['faces'])} | Gestures: {len(result['gestures'])}"
            cv2.putText(result['frame_annotated'], info_text, (10, frame.shape[0] - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            cv2.imshow('MediaPipe Face + Gesture Detection', result['frame_annotated'])
            
            # ì œìŠ¤ì²˜ ì´ë²¤íŠ¸ ì²˜ë¦¬
            for gesture in result['gestures']:
                if gesture['type'] == 'thumbs_up' and gesture['confidence'] > 0.8:
                    print(f"âœ… ì¶œì„ ì²´í¬ ì¸ì‹! (ì†: {gesture['hand']})")
                elif gesture['type'] == 'peace':
                    print(f"âœŒï¸  í™•ì¸ ì œìŠ¤ì²˜ (ì†: {gesture['hand']})")
        
        frame_count += 1
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            cv2.imwrite(f"capture_{timestamp}.jpg", frame)
            print(f"ğŸ“¸ Screenshot saved: capture_{timestamp}.jpg")
    
    cap.release()
    cv2.destroyAllWindows()
    detector.release()
