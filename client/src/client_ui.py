"""
í´ë¼ì´ì–¸íŠ¸ UI - ì¶œê²°ê´€ë¦¬ ì‹œìŠ¤í…œ
ì œìŠ¤ì²˜ ë° ì–¼êµ´ ì¸ì‹ ì¶œì„ ì²´í¬
"""

import sys
import cv2
import os
import pickle
import time
import numpy as np
import torch
import threading
from datetime import datetime
import matplotlib.ticker as ticker
from matplotlib import font_manager, rcParams
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
from dotenv import load_dotenv
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QLabel, QPushButton,
    QVBoxLayout, QHBoxLayout, QMessageBox, QFrame, QFileDialog, QStackedLayout, QInputDialog
)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal
from PyQt5.QtGui import QImage, QPixmap, QFont, QPalette, QColor

load_dotenv()

# MediaPipe import
MEDIAPIPE_AVAILABLE = False
try:
    import mediapipe as mp
    # Task API import ì‹œë„
    try:
        from mediapipe.tasks import python
        from mediapipe.tasks.python import vision
        from mediapipe import Image as MPImage
        MEDIAPIPE_AVAILABLE = True
        USE_TASK_API = True
        print("âœ“ MediaPipe Task API ì‚¬ìš© ê°€ëŠ¥")
    except:
        # Task API ì—†ìœ¼ë©´ OpenCVë§Œ ì‚¬ìš©
        USE_TASK_API = False
        MEDIAPIPE_AVAILABLE = False
        print("â„¹ï¸  MediaPipe Task APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenCVë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸  MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# SpeechBrain ìŒì„± ì¸ì‹ ëª¨ë¸ (ì˜µì…˜)
SPEECHBRAIN_AVAILABLE = False
try:
    import torchaudio

    # torchaudio backend ì²´í¬ ë¬´ë ¥í™” (Windows/í™˜ê²½ ì´ìŠˆ ëŒ€ì‘)
    def _noop(*args, **kwargs):
        return None

    if not hasattr(torchaudio, "list_audio_backends"):
        torchaudio.list_audio_backends = lambda: []
    if hasattr(torchaudio, "set_audio_backend"):
        torchaudio.set_audio_backend = _noop
    if hasattr(torchaudio, "backend") and hasattr(torchaudio.backend, "utils"):
        if hasattr(torchaudio.backend.utils, "set_audio_backend"):
            torchaudio.backend.utils.set_audio_backend = _noop
    if hasattr(torchaudio, "utils") and hasattr(torchaudio.utils, "check_torchaudio_backend"):
        torchaudio.utils.check_torchaudio_backend = _noop

    from speechbrain.inference.speaker import EncoderClassifier
    SPEECHBRAIN_AVAILABLE = True
    print("âœ“ SpeechBrain ì‚¬ìš© ê°€ëŠ¥")
except Exception:
    SPEECHBRAIN_AVAILABLE = False
    print("â„¹ï¸  SpeechBrain(ìŒì„± ëª¨ë¸)ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# UI ì„¤ì • ì„í¬íŠ¸
from lib.ui_config_lib import *


class ClientUI(QMainWindow):
    """í´ë¼ì´ì–¸íŠ¸ ì¶œì„ ì²´í¬ ìœˆë„ìš°"""

    voice_attendance_result = pyqtSignal(str, float, str)
    
    def __init__(self):
        super().__init__()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        self.camera = None
        self.current_frame = None
        self.current_mode = None  # 'gesture', 'face', 'voice', None
        self.current_user = None
        self.attendance_check_type = "in"  # 'in' or 'out'
        
        # ì–¼êµ´ ê°ì§€ê¸° ì´ˆê¸°í™”
        self.face_detector = None
        self.gesture_recognizer = None
        
        # ì–¼êµ´ ì¸ì‹ ì„¤ì •
        self.face_confidence_threshold = float(os.getenv('CONFIDENCE_THRESHOLD', 0.70))
        self.face_similarity_threshold = float(os.getenv('FACE_SIMILARITY_THRESHOLD', 0.70))
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì •
        self.gesture_confidence_threshold = float(os.getenv('GESTURE_CONFIDENCE_THRESHOLD', 0.5))
        self.gesture_cooldown = float(os.getenv('GESTURE_COOLDOWN', 3.0))
        self.last_gesture_time = {}  # {gesture_type: timestamp}
        self.detected_gestures = []  # ê°ì§€ëœ ì œìŠ¤ì²˜ íˆìŠ¤í† ë¦¬
        self.last_automation_gesture = None  # (type, confidence, timestamp)
        
        # ìŒì„± ì¸ì‹ ì„¤ì •
        self.voice_encoder = None
        self.known_voice_embeddings = []
        self.known_voice_names = []
        self.voice_similarity_threshold = float(os.getenv('VOICE_SIMILARITY_THRESHOLD', 0.7))
        self.voice_sensitivity_multiplier = float(os.getenv('VOICE_SENSITIVITY_MULTIPLIER', 3.0))
        self.voice_activity_threshold = float(os.getenv('VOICE_ACTIVITY_THRESHOLD', 0.01))
        self.voice_model_path = os.getenv('VOICE_MODEL_PATH', 'models/spkrec-ecapa-voxceleb')
        self.last_voice_result = None  # ë§ˆì§€ë§‰ ìŒì„± ì¸ì‹ ê²°ê³¼ (name, confidence)
        self.voice_result_time = 0  # ë§ˆì§€ë§‰ ìŒì„± ì¸ì‹ ì‹œê°„
        self.voice_live_running = False
        self.voice_live_interval_ms = int(os.getenv("VOICE_LIVE_INTERVAL_MS", "1500"))
        self.voice_live_duration = float(os.getenv("VOICE_LIVE_DURATION", "1.5"))
        
        if MEDIAPIPE_AVAILABLE and USE_TASK_API:
            try:
                # ì–¼êµ´ ê°ì§€ê¸°
                base_options_face = python.BaseOptions(model_asset_path='client/models/blaze_face_short_range.tflite')
                face_options = vision.FaceDetectorOptions(base_options=base_options_face)
                self.face_detector = vision.FaceDetector.create_from_options(face_options)
                
                # ì œìŠ¤ì²˜ ì¸ì‹ê¸°
                base_options_gesture = python.BaseOptions(model_asset_path='client/models/gesture_recognizer.task')
                gesture_options = vision.GestureRecognizerOptions(base_options=base_options_gesture)
                self.gesture_recognizer = vision.GestureRecognizer.create_from_options(gesture_options)
                
                print("âœ“ MediaPipe Task API ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("â„¹ï¸  ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”: models/blaze_face_short_range.tflite, models/gesture_recognizer.task")
        
        # ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”
        if SPEECHBRAIN_AVAILABLE:
            try:
                # ëª¨ë¸ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤
                from pathlib import Path
                original_symlink_to = Path.symlink_to

                def _patched_symlink_to(self, target, target_is_directory=False):
                    import shutil
                    target = Path(target)
                    self.parent.mkdir(parents=True, exist_ok=True)
                    if target.is_file():
                        shutil.copy2(target, self)
                    elif target.is_dir():
                        if self.exists():
                            shutil.rmtree(self)
                        shutil.copytree(target, self)

                Path.symlink_to = _patched_symlink_to

                try:
                    self.voice_encoder = EncoderClassifier.from_hparams(
                        source="speechbrain/spkrec-ecapa-voxceleb",
                        savedir=self.voice_model_path
                    )
                finally:
                    Path.symlink_to = original_symlink_to
                self.load_voice_data()
                print("âœ“ ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        
        # ì–¼êµ´ì¸ì‹ ë°ì´í„° ë¡œë“œ
        self.known_face_features = []
        self.known_face_names = []
        self.load_face_data()
        
        # UI ì´ˆê¸°í™”
        self.init_ui()

        # ìŒì„± ì¸ì‹ ê²°ê³¼ ì‹œê·¸ë„
        self.voice_attendance_result.connect(self.handle_voice_attendance_result)

        # ìë™ ìŒì„± ì¸ì‹ íƒ€ì´ë¨¸ (ìë™ ëª¨ë“œì—ì„œ ì£¼ê¸°ì  ì‹¤í–‰)
        self.voice_auto_timer = QTimer()
        self.voice_auto_timer.timeout.connect(self._start_voice_auto_recognition)
        self.voice_auto_interval_ms = int(os.getenv("VOICE_AUTO_INTERVAL_MS", "8000"))
        self.voice_auto_running = False

        # ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ íƒ€ì´ë¨¸ (ìŒì„± ëª¨ë“œì—ì„œ ì£¼ê¸°ì  ì‹¤í–‰)
        self.voice_live_timer = QTimer()
        self.voice_live_timer.timeout.connect(self._start_voice_live_recognition)
        
        # ì¹´ë©”ë¼ ì‹œì‘
        self.start_camera()
        
    def init_ui(self):
        """UI ì´ˆê¸°í™”"""
        # ìœˆë„ìš° ì„¤ì •
        self.setWindowTitle(WINDOW_TITLE)
        self.setGeometry(100, 100, WINDOW_WIDTH, WINDOW_HEIGHT)
        self.setStyleSheet(f"background-color: {BG_COLOR};")
        
        # ì¤‘ì•™ ìœ„ì ¯
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ (ìˆ˜í‰)
        main_layout = QHBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±
        sidebar = self.create_sidebar()
        main_layout.addWidget(sidebar)
        
        # ì¤‘ì•™ + ìš°ì¸¡ ì˜ì—­
        center_right_layout = QVBoxLayout()
        center_right_layout.setContentsMargins(20, 20, 20, 20)
        
        # ì¤‘ì•™(ì¹´ë©”ë¼) + ìš°ì¸¡(ì •ë³´) ì˜ì—­
        cam_info_layout = QHBoxLayout()
        
        # ì¹´ë©”ë¼ ì˜ì—­
        self.camera_widget = self.create_camera_view()
        cam_info_layout.addWidget(self.camera_widget)
        
        # ìš°ì¸¡ ì •ë³´ íŒ¨ë„
        right_panel = self.create_right_panel()
        cam_info_layout.addWidget(right_panel)
        
        center_right_layout.addLayout(cam_info_layout)
        
        # í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ
        gesture_guide = self.create_gesture_guide()
        center_right_layout.addWidget(gesture_guide)
        
        # í•˜ë‹¨ ìƒíƒœë°”
        status_bar = self.create_status_bar()
        center_right_layout.addWidget(status_bar)
        
        main_layout.addLayout(center_right_layout)
        
    def create_sidebar(self):
        """ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±"""
        sidebar = QWidget()
        sidebar.setFixedWidth(SIDEBAR_WIDTH)
        sidebar.setStyleSheet(f"background-color: {SIDEBAR_COLOR};")
        
        layout = QVBoxLayout(sidebar)
        layout.setContentsMargins(SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING)
        layout.setSpacing(0)
        
        # í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ ë¼ë²¨
        client_label = QLabel("ğŸ“± ì¶œì„ ì²´í¬")
        client_label.setFixedHeight(CLIENT_LABEL_HEIGHT)
        client_label.setAlignment(Qt.AlignCenter)
        client_label.setStyleSheet(f"""
            color: {TEXT_COLOR};
            font-size: {CLIENT_LABEL_FONT_SIZE}px;
            font-weight: {CLIENT_LABEL_FONT_WEIGHT};
            background-color: {ACCENT_COLOR};
            border-radius: 5px;
            padding: 10px;
        """)
        layout.addWidget(client_label)
        
        layout.addSpacing(LEFT_BUTTON_START_Y - CLIENT_LABEL_HEIGHT - SIDEBAR_PADDING)
        
        # ì¢Œì¸¡ ë²„íŠ¼ë“¤ ìƒì„±
        self.left_buttons = {}
        for btn_config in LEFT_BUTTONS:
            btn = QPushButton(btn_config["text"])
            btn.setFixedSize(LEFT_BUTTON_WIDTH, LEFT_BUTTON_HEIGHT)
            btn.setStyleSheet(self.get_button_style())
            btn.setCursor(Qt.PointingHandCursor)
            
            # ë²„íŠ¼ ì´ë²¤íŠ¸ ì—°ê²°
            btn_name = btn_config["name"]
            btn.clicked.connect(lambda checked, name=btn_name: self.on_mode_button_click(name))
            
            self.left_buttons[btn_name] = btn
            layout.addWidget(btn)
            layout.addSpacing(LEFT_BUTTON_SPACING)
        
        layout.addStretch()
        
        return sidebar
    
    def create_camera_view(self):
        """ì¹´ë©”ë¼ ë·° + ì¶œì„ í˜„í™© í”Œë¡¯ ì˜ì—­ ìƒì„±"""
        container = QWidget()
        layout = QVBoxLayout(container)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(8)

        # ì¶œì„ í˜„í™© íˆ´ë°” (í”Œë¡¯ ìœ„)
        self.attendance_toolbar = QWidget()
        toolbar_layout = QHBoxLayout(self.attendance_toolbar)
        toolbar_layout.setContentsMargins(0, 0, 0, 0)
        toolbar_layout.setSpacing(10)

        self.today_attendance_btn = QPushButton("ê¸ˆì¼ ì¶œì„ ì¡°íšŒ")
        self.today_attendance_btn.setFixedHeight(30)
        self.today_attendance_btn.setStyleSheet(self.get_button_style())
        self.today_attendance_btn.setCursor(Qt.PointingHandCursor)
        self.today_attendance_btn.clicked.connect(self._on_today_attendance_clicked)

        self.date_attendance_btn = QPushButton("ë‚ ì§œë³„ ì¡°íšŒ")
        self.date_attendance_btn.setFixedHeight(30)
        self.date_attendance_btn.setStyleSheet(self.get_button_style())
        self.date_attendance_btn.setCursor(Qt.PointingHandCursor)
        self.date_attendance_btn.clicked.connect(self._on_date_attendance_clicked)

        self.range_attendance_btn = QPushButton("ê¸°ê°„ ì¡°íšŒ")
        self.range_attendance_btn.setFixedHeight(30)
        self.range_attendance_btn.setStyleSheet(self.get_button_style())
        self.range_attendance_btn.setCursor(Qt.PointingHandCursor)
        self.range_attendance_btn.clicked.connect(self._on_range_attendance_clicked)

        toolbar_layout.addWidget(self.today_attendance_btn)
        toolbar_layout.addWidget(self.date_attendance_btn)
        toolbar_layout.addWidget(self.range_attendance_btn)
        toolbar_layout.addStretch()

        layout.addWidget(self.attendance_toolbar)

        # ìŠ¤íƒ ì˜ì—­ (ì¹´ë©”ë¼/í”Œë¡¯)
        self.camera_stack = QStackedLayout()
        layout.addLayout(self.camera_stack)

        # ì¹´ë©”ë¼ ë¼ë²¨
        self.camera_label = QLabel()
        self.camera_label.setFixedSize(CAM_WIDTH, CAM_HEIGHT)
        self.camera_label.setAlignment(Qt.AlignCenter)
        self.camera_label.setStyleSheet(f"""
            background-color: {CAM_BG_COLOR};
            border: 3px solid {ACCENT_COLOR};
            border-radius: 10px;
        """)
        self.camera_label.setText("ğŸ“¹ ì¹´ë©”ë¼ ë¡œë”© ì¤‘...")
        self.camera_label.setFont(QFont("Arial", 14))
        self.camera_label.setStyleSheet(self.camera_label.styleSheet() + f"color: {TEXT_COLOR};")

        # í”Œë¡¯ ìº”ë²„ìŠ¤
        self.plot_container = QWidget()
        plot_layout = QVBoxLayout(self.plot_container)
        plot_layout.setContentsMargins(0, 0, 0, 0)
        plot_layout.setSpacing(0)
        self.plot_figure = Figure(figsize=(6, 4), dpi=100)
        self.plot_canvas = FigureCanvas(self.plot_figure)
        self.plot_canvas.setFixedSize(CAM_WIDTH, CAM_HEIGHT)
        plot_layout.addWidget(self.plot_canvas)

        self.camera_stack.addWidget(self.camera_label)
        self.camera_stack.addWidget(self.plot_container)
        self.camera_stack.setCurrentWidget(self.camera_label)
        self.attendance_toolbar.setVisible(False)

        return container
    
    def create_right_panel(self):
        """ìš°ì¸¡ ì •ë³´ íŒ¨ë„ ìƒì„±"""
        panel = QWidget()
        panel.setFixedWidth(RIGHT_PANEL_WIDTH)
        
        layout = QVBoxLayout(panel)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(15)
        
        # ì‚¬ìš©ì ì •ë³´ í”„ë ˆì„
        user_info_frame = QFrame()
        user_info_frame.setFixedHeight(USER_INFO_HEIGHT)
        user_info_frame.setStyleSheet(f"""
            background-color: {USER_INFO_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        user_info_layout = QVBoxLayout(user_info_frame)
        
        user_title = QLabel("ğŸ‘¤ ì‚¬ìš©ì ì •ë³´")
        user_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        user_info_layout.addWidget(user_title)
        
        self.user_name_label = QLabel("ì´ë¦„: -")
        self.user_name_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_name_label)
        
        self.user_id_label = QLabel("í•™ë²ˆ: -")
        self.user_id_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_id_label)
        
        user_info_layout.addStretch()
        
        layout.addWidget(user_info_frame)
        
        # ì¶œì„ ìƒíƒœ í”„ë ˆì„
        status_frame = QFrame()
        status_frame.setFixedHeight(ATTENDANCE_STATUS_HEIGHT)
        status_frame.setStyleSheet(f"""
            background-color: {ATTENDANCE_STATUS_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        status_layout = QVBoxLayout(status_frame)
        
        status_title = QLabel("ğŸ“Š ì¶œì„ ìƒíƒœ")
        status_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        status_layout.addWidget(status_title)
        
        self.attendance_status_label = QLabel("ëŒ€ê¸° ì¤‘...")
        self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
        self.attendance_status_label.setAlignment(Qt.AlignCenter)
        status_layout.addWidget(self.attendance_status_label)

        self.check_type_container = QWidget()
        check_type_layout = QHBoxLayout(self.check_type_container)
        check_type_layout.setContentsMargins(0, 0, 0, 0)
        check_type_layout.setSpacing(8)

        self.check_in_btn = QPushButton("ì…ì‹¤")
        self.check_out_btn = QPushButton("í‡´ì‹¤")
        for btn in (self.check_in_btn, self.check_out_btn):
            btn.setFixedHeight(26)
            btn.setCursor(Qt.PointingHandCursor)
            btn.setStyleSheet(self.get_button_style(font_size=11))

        self.check_in_btn.clicked.connect(lambda: self._set_check_type("in"))
        self.check_out_btn.clicked.connect(lambda: self._set_check_type("out"))

        check_type_layout.addWidget(self.check_in_btn)
        check_type_layout.addWidget(self.check_out_btn)
        status_layout.addWidget(self.check_type_container)
        self.check_type_container.setVisible(False)
        
        self.detected_gesture_label = QLabel("ì œìŠ¤ì²˜: -")
        self.detected_gesture_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        status_layout.addWidget(self.detected_gesture_label)
        
        status_layout.addStretch()
        
        layout.addWidget(status_frame)
        
        layout.addStretch()
        
        return panel
    
    def create_gesture_guide(self):
        """í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ ìƒì„±"""
        guide_frame = QFrame()
        guide_frame.setFixedHeight(GESTURE_GUIDE_HEIGHT)
        guide_frame.setStyleSheet(f"""
            background-color: {GESTURE_GUIDE_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        layout = QVBoxLayout(guide_frame)
        
        title = QLabel("ğŸ‘‹ ì œìŠ¤ì²˜ ê°€ì´ë“œ")
        title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 13px; font-weight: bold;")
        layout.addWidget(title)
        
        # ì œìŠ¤ì²˜ ëª©ë¡
        gestures_layout = QHBoxLayout()
        
        for gesture_name, gesture_info in GESTURES.items():
            gesture_widget = QLabel(f"{gesture_info['emoji']}\n{gesture_info['text']}")
            gesture_widget.setAlignment(Qt.AlignCenter)
            gesture_widget.setStyleSheet(f"""
                color: {gesture_info['color']};
                font-size: 11px;
                padding: 5px;
            """)
            gestures_layout.addWidget(gesture_widget)
        
        layout.addLayout(gestures_layout)
        
        return guide_frame
    
    def create_status_bar(self):
        """í•˜ë‹¨ ìƒíƒœë°” ìƒì„±"""
        status_bar = QLabel("âœ… ì¤€ë¹„ ì™„ë£Œ - ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
        status_bar.setFixedHeight(STATUS_BAR_HEIGHT)
        status_bar.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)
        status_bar.setStyleSheet(f"""
            background-color: {STATUS_BAR_BG_COLOR};
            color: {TEXT_COLOR};
            font-size: {STATUS_FONT_SIZE}px;
            padding-left: 15px;
            border-radius: 5px;
        """)
        self.status_bar = status_bar
        return status_bar
    
    def get_button_style(self, font_size=LEFT_BUTTON_FONT_SIZE):
        """ë²„íŠ¼ ìŠ¤íƒ€ì¼ ë°˜í™˜"""
        return f"""
            QPushButton {{
                background-color: {BUTTON_COLOR};
                color: {TEXT_COLOR};
                border: none;
                border-radius: 5px;
                font-size: {font_size}px;
                font-weight: bold;
                padding: 5px;
            }}
            QPushButton:hover {{
                background-color: {BUTTON_HOVER_COLOR};
            }}
            QPushButton:pressed {{
                background-color: #1B1464;
            }}
        """
    
    def start_camera(self):
        """ì¹´ë©”ë¼ ì‹œì‘"""
        self.camera = cv2.VideoCapture(CAMERA_INDEX)
        
        if not self.camera.isOpened():
            self.update_status("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì¹´ë©”ë¼ í•´ìƒë„ ì„¤ì •
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, CAM_WIDTH)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, CAM_HEIGHT)
        
        # íƒ€ì´ë¨¸ë¡œ í”„ë ˆì„ ì—…ë°ì´íŠ¸
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(int(1000 / CAMERA_FPS))
        
        self.update_status("ğŸ“¹ ì¹´ë©”ë¼ í™œì„±í™”ë¨")
    
    def load_voice_data(self):
        """ì €ì¥ëœ ìŒì„±(ì„ë² ë”©) ë°ì´í„° ë¡œë“œ"""
        # Resolve path relative to this file to the repository-level data/voice
        voice_data_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'voice', 'voice_embeddings.pkl'))

        if os.path.exists(voice_data_file):
            try:
                with open(voice_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_voice_embeddings = data.get('embeddings', [])
                    self.known_voice_names = data.get('names', [])
                print(f"âœ“ {len(self.known_voice_names)}ëª…ì˜ ìŒì„± ë°ì´í„° ë¡œë“œë¨ ({voice_data_file})")
                if self.known_voice_embeddings:
                    first_vec = np.array(self.known_voice_embeddings[0]).flatten()
                    first_norm = float(np.linalg.norm(first_vec)) if first_vec.size > 0 else 0.0
                    print(f"ğŸ”¬ ë“±ë¡ ì„ë² ë”©[0] norm: {first_norm:.6f}, shape: {first_vec.shape}")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"â„¹ï¸  ë“±ë¡ëœ ìŒì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ({voice_data_file})")
    
    def save_voice_data(self):
        """ìŒì„± ì„ë² ë”© ì €ì¥"""
        voice_data_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'voice', 'voice_embeddings.pkl'))
        os.makedirs(os.path.dirname(voice_data_file), exist_ok=True)

        data = {
            'embeddings': self.known_voice_embeddings,
            'names': self.known_voice_names
        }

        try:
            with open(voice_data_file, 'wb') as f:
                pickle.dump(data, f)
            print("âœ“ ìŒì„± ë°ì´í„° ì €ì¥ë¨")
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def extract_voice_embedding(self, audio_file):
        """ìŒì„± íŒŒì¼ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            import soundfile as sf
            from scipy.signal import resample

            audio, sr = sf.read(audio_file, dtype='float32')
            if audio.ndim > 1:
                audio = audio[:, 0]

            if sr != 16000:
                num_samples = int(len(audio) * 16000 / sr)
                audio = resample(audio, num_samples)

            signal = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)
            emb = self.voice_encoder.encode_batch(signal)
            return emb.detach().cpu().numpy()
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def recognize_voice(self, audio_file):
        """ìŒì„± íŒŒì¼ ì¸ì‹"""
        embedding = self.extract_voice_embedding(audio_file)
        
        if embedding is None:
            return "Unknown", 0.0

        embedding = np.array(embedding).flatten()
        emb_norm = float(np.linalg.norm(embedding)) if embedding.size > 0 else 0.0
        print(f"ğŸ”¬ ì¶”ì¶œ ì„ë² ë”© norm: {emb_norm:.6f}, shape: {embedding.shape}")
        if not np.isfinite(emb_norm) or emb_norm == 0.0:
            print("âš ï¸  ì¶”ì¶œ ì„ë² ë”©ì´ ë¹„ì •ìƒì…ë‹ˆë‹¤ (NaN/0).")
            return "Unknown", 0.0
        
        best_match_name = "Unknown"
        best_similarity = 0.0
        
        # ì €ì¥ëœ ìŒì„±ê³¼ ë¹„êµ
        for known_emb, known_name in zip(self.known_voice_embeddings, self.known_voice_names):
            known_vec = np.array(known_emb).flatten()
            known_norm = float(np.linalg.norm(known_vec)) if known_vec.size > 0 else 0.0
            if not np.isfinite(known_norm) or known_norm == 0.0:
                print(f"âš ï¸  ë“±ë¡ ì„ë² ë”© ë¹„ì •ìƒ: {known_name} (norm={known_norm})")
                continue

            similarity = self.cosine_similarity(embedding, known_vec)
            similarity = min(float(similarity) * self.voice_sensitivity_multiplier, 1.0)
            if not np.isfinite(similarity):
                print(f"âš ï¸  ìœ ì‚¬ë„ NaN: {known_name}")
                continue
            print(f"ğŸ” í›„ë³´: {known_name} | sim={similarity:.6f} | norm={known_norm:.6f}")
            if similarity > best_similarity:
                best_similarity = float(similarity)
                best_match_name = known_name
        
        # ì„ê³„ê°’ ë¯¸ë‹¬ì´ë©´ ì´ë¦„ë§Œ Unknown ì²˜ë¦¬ (ìœ ì‚¬ë„ëŠ” ìœ ì§€)
        if best_similarity < self.voice_similarity_threshold:
            best_match_name = "Unknown"
        
        return best_match_name, float(best_similarity)

    def select_input_device(self, sd):
        """ì…ë ¥ ë§ˆì´í¬ ì„ íƒ (VOICE_INPUT_DEVICE í™˜ê²½ë³€ìˆ˜ ìš°ì„ )"""
        try:
            preferred = os.getenv("VOICE_INPUT_DEVICE", "WO Mic").strip()
            devices = sd.query_devices()

            # ìˆ«ì ì§€ì • (ì¥ì¹˜ ì¸ë±ìŠ¤)
            if preferred.isdigit():
                idx = int(preferred)
                if 0 <= idx < len(devices) and devices[idx]["max_input_channels"] > 0:
                    return idx, devices[idx]["name"]

            # ì´ë¦„ ë¶€ë¶„ ì¼ì¹˜
            preferred_lower = preferred.lower()
            for i, device in enumerate(devices):
                if device.get("max_input_channels", 0) > 0:
                    if preferred_lower in device["name"].lower():
                        return i, device["name"]

            # ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜ fallback
            for i, device in enumerate(devices):
                if device.get("max_input_channels", 0) > 0:
                    return i, device["name"]
        except Exception as e:
            print(f"âš ï¸  ì…ë ¥ ì¥ì¹˜ ì„ íƒ ì‹¤íŒ¨: {e}")

        return None, None

    def record_voice_and_recognize(self, duration=3, sample_rate=16000):
        """ë§ˆì´í¬ì—ì„œ ìŒì„± ë…¹ìŒ í›„ ì¸ì‹"""
        return self.record_voice_and_recognize_internal(duration, sample_rate, ui_updates=True)

    def record_voice_and_recognize_internal(self, duration=3, sample_rate=16000, ui_updates=True):
        """ë§ˆì´í¬ì—ì„œ ìŒì„± ë…¹ìŒ í›„ ì¸ì‹ (UI ì—…ë°ì´íŠ¸ ì˜µì…˜)"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "Unknown", 0.0, "SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        if not self.known_voice_embeddings:
            if ui_updates:
                self.update_status("âš ï¸  ë“±ë¡ëœ ìŒì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "Unknown", 0.0, "ë“±ë¡ëœ ìŒì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        try:
            import sounddevice as sd
            import soundfile as sf

            temp_audio_file = "./tmp_voice_attendance.wav"

            # ì…ë ¥ ì¥ì¹˜ ì„ íƒ (í™˜ê²½ë³€ìˆ˜ VOICE_INPUT_DEVICE ìš°ì„ )
            device_id, device_name = self.select_input_device(sd)
            if device_id is None:
                self.update_status("âŒ ë§ˆì´í¬ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return "Unknown", 0.0
            print(f"ğŸ™ï¸  ì‚¬ìš© ë§ˆì´í¬: [{device_id}] {device_name}")

            # ë…¹ìŒ
            if ui_updates:
                self.update_status(f"ğŸ¤ ìŒì„± ë…¹ìŒ ì¤‘... ({duration}ì´ˆ)")
            audio_data = sd.rec(
                int(duration * sample_rate),
                samplerate=sample_rate,
                channels=1,
                dtype='float32',
                device=device_id
            )
            sd.wait()

            # ìŒì„± ì—ë„ˆì§€ í™•ì¸
            rms = float(np.sqrt(np.mean(np.square(audio_data)))) if audio_data is not None else 0.0
            print(f"ğŸ”Š ë…¹ìŒ RMS: {rms:.6f}")
            if rms < self.voice_activity_threshold:
                if ui_updates:
                    self.update_status("ğŸ”‡ ìŒì„± ë¯¸ê°ì§€")
                return "Unknown", 0.0, "no_voice"

            # íŒŒì¼ë¡œ ì €ì¥
            sf.write(temp_audio_file, audio_data, sample_rate)

            # ìŒì„± ì¸ì‹ ì‹¤í–‰
            name, confidence = self.recognize_voice(temp_audio_file)

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_audio_file):
                os.remove(temp_audio_file)

            return name, confidence, ""
        except ImportError:
            if ui_updates:
                self.update_status("âŒ ìŒì„± ë…¹ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                QMessageBox.warning(
                    self,
                    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜",
                    "sounddevice ë° soundfileì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\nì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
                )
            return "Unknown", 0.0, "sounddevice/soundfile ë¯¸ì„¤ì¹˜"
        except Exception as e:
            if ui_updates:
                self.update_status(f"âŒ ìŒì„± ë…¹ìŒ ì˜¤ë¥˜: {str(e)}")
                QMessageBox.warning(
                    self,
                    "ë…¹ìŒ ì˜¤ë¥˜",
                    f"ìŒì„± ë…¹ìŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}"
                )
            return "Unknown", 0.0, str(e)
    
    def record_voice_and_extract(self, filename="./tmp_voice.wav", duration=3, fs=16000):
        """ìŒì„± ë…¹ìŒ ë° ì„ë² ë”© ì¶”ì¶œ (placeholder)"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        try:
            import soundfile as sf
            from scipy.signal import resample

            audio, sr = sf.read(filename, dtype='float32')
            if audio.ndim > 1:
                audio = audio[:, 0]

            if sr != 16000:
                num_samples = int(len(audio) * 16000 / sr)
                audio = resample(audio, num_samples)

            signal = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)
            emb = self.voice_encoder.encode_batch(signal)
            return emb.detach().cpu().numpy()
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def process_voice_event(self, name, confidence):
        """ê°ì§€ëœ ìŒì„± ì²˜ë¦¬"""
        self.user_name_label.setText(f"ì´ë¦„: {name}")
        self.attendance_status_label.setText(f"ìŒì„± ì¸ì‹ë¨ ({confidence:.1%})")
        self.attendance_status_label.setStyleSheet(
            f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
        )
        print(f"ğŸ“¤ ìŒì„± ì´ë²¤íŠ¸: {name} ({confidence:.1%})")
    
    def load_face_data(self):
        """ì €ì¥ëœ ì–¼êµ´ ë°ì´í„° ë¡œë“œ"""
        # Resolve path relative to this file to reach the repository-level data directory
        face_data_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'face_data.pkl'))

        if os.path.exists(face_data_file):
            try:
                with open(face_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_face_features = data.get('features', [])
                    self.known_face_names = data.get('names', [])
                print(f"âœ“ {len(self.known_face_names)}ëª…ì˜ ì–¼êµ´ ë°ì´í„° ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ì–¼êµ´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"â„¹ï¸  ë“±ë¡ëœ ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (checked {face_data_file})")
    
    def extract_face_features(self, detection, image_width, image_height):
        """ì–¼êµ´ ê°ì§€ ê²°ê³¼ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
        bbox = detection.bounding_box
        features = [
            bbox.origin_x / image_width,
            bbox.origin_y / image_height,
            bbox.width / image_width,
            bbox.height / image_height
        ]
        return np.array(features)
    
    def cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    def recognize_faces(self, frame):
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ì¸ì‹ (MediaPipe Task API ê¸°ë°˜)"""
        if not self.face_detector:
            # MediaPipeê°€ ì—†ìœ¼ë©´ OpenCV Haar Cascade ì‚¬ìš©
            return self.recognize_faces_opencv(frame)
        
        # MediaPipe Image ê°ì²´ ìƒì„±
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # ì–¼êµ´ ê°ì§€
        detection_result = self.face_detector.detect(mp_image)
        
        recognized_names = []
        h, w, _ = frame.shape
        
        if detection_result.detections:
            for detection in detection_result.detections:
                # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
                bbox = detection.bounding_box
                x_min = int(bbox.origin_x)
                y_min = int(bbox.origin_y)
                x_max = int(bbox.origin_x + bbox.width)
                y_max = int(bbox.origin_y + bbox.height)
                
                # ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œ
                current_features = self.extract_face_features(detection, w, h)
                
                # ë“±ë¡ëœ ì–¼êµ´ê³¼ ë¹„êµ
                best_match_name = "Unknown"
                best_similarity = 0
                
                for known_features, known_name in zip(self.known_face_features, self.known_face_names):
                    similarity = self.cosine_similarity(current_features, known_features)
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match_name = known_name
                
                # ìœ ì‚¬ë„ ì„ê³„ê°’ (ì„¤ì •ê°’ ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ)
                confidence = best_similarity * 100
                if (best_similarity * 100) < (self.face_similarity_threshold * 100):
                    best_match_name = "Unknown"
                    confidence = 0
                
                if best_match_name != "Unknown":
                    recognized_names.append((best_match_name, confidence))
                
                # ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                color = (0, 255, 0) if best_match_name != "Unknown" else (0, 0, 255)
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 2)
                
                # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ëˆˆ, ì½”, ì… ë“±)
                for keypoint in detection.keypoints:
                    kp_x = int(keypoint.x * w)
                    kp_y = int(keypoint.y * h)
                    cv2.circle(frame, (kp_x, kp_y), 2, (0, 255, 255), -1)
                
                # ì´ë¦„ê³¼ ì‹ ë¢°ë„ í‘œì‹œ
                label_height = 40
                cv2.rectangle(frame, (x_min, y_max), (x_max, y_max + label_height), color, cv2.FILLED)
                
                if best_match_name != "Unknown":
                    cv2.putText(frame, best_match_name, (x_min + 6, y_max + 15), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                    cv2.putText(frame, f"{confidence:.1f}%", (x_min + 6, y_max + 35), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255), 1)
                else:
                    cv2.putText(frame, "Unknown", (x_min + 6, y_max + 25), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
        
        return frame, recognized_names
    
    def recognize_faces_opencv(self, frame):
        """í´ë°±: OpenCV Haar Cascadeë¡œ ì–¼êµ´ ê°ì§€"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Haar Cascade ë¶ˆëŸ¬ì˜¤ê¸°
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        recognized_names = []
        
        for (x, y, w, h) in faces:
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(frame, "Face Detected", (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        return frame, recognized_names
    
    def recognize_gesture(self, frame, skip_cooldown=False):
        """í”„ë ˆì„ì—ì„œ ì œìŠ¤ì²˜ ì¸ì‹ (Quality Gate í¬í•¨)
        
        Args:
            frame: ì…ë ¥ í”„ë ˆì„
            skip_cooldown: Trueì¼ ê²½ìš° ì¿¨ë‹¤ìš´ ë¬´ì‹œ (ìë™ ì¸ì‹ ëª¨ë“œìš©)
        """
        if not MEDIAPIPE_AVAILABLE or not self.gesture_recognizer:
            return [], frame
        
        try:
            # MediaPipe Image ê°ì²´ ìƒì„±
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
            
            # ì œìŠ¤ì²˜ ì¸ì‹
            result = self.gesture_recognizer.recognize(mp_image)
            
            detected_gestures = []
            current_time = time.time()
            
            if hasattr(result, 'gestures') and result.gestures:
                for gesture_list in result.gestures:
                    if gesture_list:  # ì œìŠ¤ì²˜ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´
                        gesture = gesture_list[0]  # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ì œìŠ¤ì²˜
                        gesture_name = gesture.category_name
                        confidence = gesture.score
                        
                        # Quality Gate: ì‹ ë¢°ë„ ì„ê³„ê°’ í™•ì¸
                        if confidence < self.gesture_confidence_threshold:
                            continue
                        
                        # Cooldown í™•ì¸: ê°™ì€ ì œìŠ¤ì²˜ê°€ ìµœê·¼ì— ê°ì§€ë˜ì—ˆëŠ”ê°€?
                        # (ìë™ ì¸ì‹ ëª¨ë“œì—ì„œëŠ” cooldown ë¬´ì‹œ)
                        if not skip_cooldown:
                            if gesture_name in self.last_gesture_time:
                                if current_time - self.last_gesture_time[gesture_name] < self.gesture_cooldown:
                                    continue  # ì¿¨ë‹¤ìš´ ì¤‘ì´ë©´ ë¬´ì‹œ
                        
                        # ìœ íš¨í•œ ì œìŠ¤ì²˜: ì—…ë°ì´íŠ¸
                        self.last_gesture_time[gesture_name] = current_time
                        detected_gestures.append({
                            'type': gesture_name,
                            'confidence': confidence,
                            'timestamp': current_time
                        })
                        
                        print(f"âœ“ ì œìŠ¤ì²˜ ì¸ì‹: {gesture_name} ({confidence:.2f})")
            
            return detected_gestures, frame
            
        except Exception as e:
            print(f"âš ï¸  ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
            return [], frame
    
    def process_gesture_event(self, gesture_data):
        """ê°ì§€ëœ ì œìŠ¤ì²˜ ì²˜ë¦¬"""
        gesture_type = gesture_data['type']
        confidence = gesture_data['confidence']
        
        self.detected_gesture_label.setText(f"ì œìŠ¤ì²˜: {gesture_type} ({confidence:.1%})")
        
        # ZeroMQë¡œ ì„œë²„ì— ì „ì†¡ (í•„ìš”ì‹œ)
        print(f"ğŸ“¤ ì œìŠ¤ì²˜ ì´ë²¤íŠ¸: {gesture_type} ({confidence:.1%})")
    
    def update_frame(self):
        """ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
        ret, frame = self.camera.read()
        
        if ret:
            self.current_frame = frame
            display_frame = frame.copy()
            
            # ì–¼êµ´ ì¸ì‹ ëª¨ë“œ
            if self.current_mode == "face_attendance":
                display_frame, recognized_names = self.recognize_faces(display_frame)
                
                if recognized_names:
                    name, confidence = recognized_names[0]
                    self.user_name_label.setText(f"ì´ë¦„: {name}")
                    self.attendance_status_label.setText(f"ì¸ì‹ë¨ ({confidence:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    
                    if confidence > 80:
                        self.detected_gesture_label.setText(f"âœ“ {name} ì¶œì„ í™•ì¸")
                else:
                    self.detected_gesture_label.setText("ì–¼êµ´: ê°ì§€ ì•ˆë¨")
            
            # ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë“œ
            elif self.current_mode == "gesture_attendance":
                gestures, display_frame = self.recognize_gesture(display_frame)
                
                # ì œìŠ¤ì²˜ ì˜¤ë²„ë ˆì´ í‘œì‹œ
                overlay_y = 30
                if gestures:
                    for gesture in gestures:
                        self.process_gesture_event(gesture)
                        gesture_type = gesture['type']
                        confidence = gesture['confidence']
                        
                        gesture_text = f"Gesture: {gesture_type} ({confidence*100:.1f}%)"
                        cv2.putText(display_frame, gesture_text, (10, overlay_y), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        overlay_y += 30
                        
                        self.attendance_status_label.setText(f"ì œìŠ¤ì²˜ ê°ì§€ë¨!")
                        self.attendance_status_label.setStyleSheet(
                            f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                        )
                else:
                    cv2.putText(display_frame, "Gesture: Waiting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                    self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
                    self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
            # ìë™ ì¸ì‹ ëª¨ë“œ (ì–¼êµ´ + ì œìŠ¤ì²˜ + ìŒì„± ë™ì‹œ ì¸ì‹)
            elif self.current_mode == "automation":
                display_frame, face_results = self.recognize_faces(display_frame)
                # ìë™ ì¸ì‹ ëª¨ë“œì—ì„œëŠ” ì œìŠ¤ì²˜ ì¿¨ë‹¤ìš´ ë¬´ì‹œ
                gestures, display_frame = self.recognize_gesture(display_frame, skip_cooldown=True)
                
                # Decision Fusion Logic: ì–¼êµ´ + ì œìŠ¤ì²˜ ì¢…í•© íŒë‹¨
                face_score = 0
                face_name = "Unknown"
                
                if face_results:
                    face_name, face_confidence = face_results[0]
                    face_score = face_confidence / 100.0
                
                gesture_score = 0
                gesture_detected = None

                if gestures:
                    gesture_detected = gestures[0]['type']
                    gesture_score = gestures[0]['confidence']
                    self.last_automation_gesture = (gesture_detected, gesture_score, time.time())
                elif self.last_automation_gesture:
                    gesture_detected, gesture_score, _ = self.last_automation_gesture
                
                # í†µí•© ì ìˆ˜ ê³„ì‚° (ì–¼êµ´ 70%, ì œìŠ¤ì²˜ 30%)
                fusion_score = (face_score * 0.7) + (gesture_score * 0.3)
                
                # ë™ì‹œ ì˜¤ë²„ë ˆì´: ì¹´ë©”ë¼ í”„ë ˆì„ì— ì‹¤ì‹œê°„ í‘œì‹œ
                #h, w, _ = display_frame.shape
                
                # 1. ì–¼êµ´ ì •ë³´ ì˜¤ë²„ë ˆì´
                overlay_y = 30
                if face_name != "Unknown":
                    face_text = f"Face: {face_name} ({face_score*100:.1f}%)"
                    cv2.putText(display_frame, face_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    overlay_y += 30
                else:
                    cv2.putText(display_frame, "Face: Detecting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
                    overlay_y += 30
                
                # 2. ì œìŠ¤ì²˜ ì •ë³´ ì˜¤ë²„ë ˆì´
                if gesture_detected:
                    gesture_text = f"Gesture: {gesture_detected} ({gesture_score*100:.1f}%)"
                    cv2.putText(display_frame, gesture_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    overlay_y += 30
                else:
                    cv2.putText(display_frame, "Gesture: Waiting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                    overlay_y += 30
                
                # 3. ìŒì„± ì •ë³´ ì˜¤ë²„ë ˆì´ (ìµœê·¼ 5ì´ˆ ì´ë‚´ ê²°ê³¼ í‘œì‹œ)
                current_time = time.time()
                voice_name = "Unknown"
                voice_score = 0.0
                voice_score_for_fusion = 0.0
                
                if self.last_voice_result and (current_time - self.voice_result_time) < 5.0:
                    voice_name, voice_score = self.last_voice_result
                    voice_score_for_fusion = voice_score
                    voice_text = f"Voice: {voice_name} ({voice_score:.2f})"
                    cv2.putText(display_frame, voice_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    cv2.putText(display_frame, "Voice: Listening...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                overlay_y += 30
                
                # 4. ì¢…í•© ì ìˆ˜ ì˜¤ë²„ë ˆì´ (ì–¼êµ´ 50%, ì œìŠ¤ì²˜ 25%, ìŒì„± 25%)
                fusion_score = (face_score * 0.5) + (gesture_score * 0.25) + (voice_score_for_fusion * 0.25)
                fusion_text = f"Fusion Score: {fusion_score*100:.1f}%"
                cv2.putText(display_frame, fusion_text, (10, overlay_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # ìë™ ì¸ì‹ ê¸°ì¤€: ì–¼êµ´ ì¸ì‹ë¥  > 70% ë˜ëŠ” (ì–¼êµ´ > 50% AND ì œìŠ¤ì²˜ ê°ì§€)
                if fusion_score >= 0.90:
                    display_name = face_name if face_name != "Unknown" else voice_name
                    if display_name == "Unknown":
                        display_name = "ì‚¬ìš©ì"
                    self.user_name_label.setText(f"ì´ë¦„: {display_name}")
                    self.attendance_status_label.setText(f"âœ“ {display_name} ì¶œì„ ì™„ë£Œ")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    status_msg = f"Fusion Score: {fusion_score*100:.1f}%"
                    if gesture_detected:
                        status_msg += f" + ì œìŠ¤ì²˜: {gesture_detected}"
                    if voice_score_for_fusion > 0.0:
                        status_msg += f" + ìŒì„±: {voice_score_for_fusion*100:.1f}%"
                    self.detected_gesture_label.setText(status_msg)
                    self._mark_attendance_if_needed(display_name, fusion_score)
                elif face_name != "Unknown" and face_score > 0.7:
                    # ì–¼êµ´ ì¸ì‹ ì„±ê³µ
                    self.user_name_label.setText(f"ì´ë¦„: {face_name}")
                    self.attendance_status_label.setText(f"âœ“ Fusion Score ({fusion_score*100:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    
                    status_msg = f"âœ“ {face_name} - ì–¼êµ´: {face_score*100:.1f}%"
                    if gesture_detected:
                        status_msg += f" + ì œìŠ¤ì²˜: {gesture_detected}"
                    self.detected_gesture_label.setText(status_msg)
                    
                elif face_name != "Unknown" and face_score > 0.5 and gesture_detected:
                    # ì–¼êµ´ + ì œìŠ¤ì²˜ ì¡°í•©ìœ¼ë¡œ ì¸ì‹
                    self.user_name_label.setText(f"ì´ë¦„: {face_name}")
                    self.attendance_status_label.setText(f"âœ“ ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹ ({fusion_score*100:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    self.detected_gesture_label.setText(f"ì–¼êµ´: {face_score*100:.1f}% + ì œìŠ¤ì²˜: {gesture_detected}")
                    if fusion_score >= 0.80:
                        self._mark_attendance_if_needed(face_name, fusion_score)
                else:
                    # ëŒ€ê¸°
                    self.attendance_status_label.setText("ìë™ ì¸ì‹ ì¤‘...")
                    self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
                    
                    status_msg = ""
                    if face_name != "Unknown":
                        status_msg = f"ì–¼êµ´: {face_score*100:.1f}%"
                    if gesture_detected:
                        if status_msg:
                            status_msg += f" + ì œìŠ¤ì²˜: {gesture_detected}"
                        else:
                            status_msg = f"ì œìŠ¤ì²˜: {gesture_detected}"
                    
                    if voice_score_for_fusion > 0.0:
                        status_msg = f"{status_msg} + ìŒì„±: {voice_score_for_fusion*100:.1f}%" if status_msg else f"ìŒì„±: {voice_score_for_fusion*100:.1f}%"

                    if status_msg:
                        self.detected_gesture_label.setText(status_msg)
                    else:
                        self.detected_gesture_label.setText("ëŒ€ê¸° ì¤‘...")
            
            # ëª¨ë“œ í‘œì‹œ
            if self.current_mode:
                mode_text = {
                    "gesture_attendance": "ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ",
                    "face_attendance": "ì–¼êµ´ ì¸ì‹ ëª¨ë“œ",
                    "automation": "ğŸ§  ìë™ ì¸ì‹ ëª¨ë“œ (ì–¼êµ´+ì œìŠ¤ì²˜+ìŒì„±)",
                }
                cv2.putText(display_frame, mode_text.get(self.current_mode, ""), 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # BGRì„ RGBë¡œ ë³€í™˜
            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            
            # QImageë¡œ ë³€í™˜
            h, w, ch = rgb_frame.shape
            bytes_per_line = ch * w
            qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
            
            # QLabelì— í‘œì‹œ
            pixmap = QPixmap.fromImage(qt_image)
            scaled_pixmap = pixmap.scaled(CAM_WIDTH, CAM_HEIGHT, Qt.KeepAspectRatio)
            self.camera_label.setPixmap(scaled_pixmap)
    
    def on_mode_button_click(self, mode_name):
        """ëª¨ë“œ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸"""
        self.current_mode = mode_name
        if self.voice_auto_timer.isActive() and mode_name != "automation":
            self.voice_auto_timer.stop()
        if self.voice_live_timer.isActive() and mode_name != "voice_attendance":
            self.voice_live_timer.stop()
        
        if mode_name == "gesture_attendance":
            self._show_camera_view()
            self.update_status("âœ‹ ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "face_attendance":
            self._show_camera_view()
            self.update_status("ğŸ˜Š ì–¼êµ´ ì¸ì‹ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì–¼êµ´ ì¸ì‹ ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "voice_attendance":
            self._show_camera_view()
            self.update_status("ğŸ¤ ìŒì„± ì¸ì‹ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ìŒì„± ë…¹ìŒ ëŒ€ê¸° ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            # ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘ (ì§§ì€ êµ¬ê°„ ë°˜ë³µ)
            self.voice_live_timer.start(self.voice_live_interval_ms)
            
        elif mode_name == "automation":
            self._show_camera_view()
            self.update_status("ğŸ§  ìë™ ì¸ì‹ ëª¨ë“œ í™œì„±í™” (ì–¼êµ´+ì œìŠ¤ì²˜+ìŒì„±)")
            self.attendance_status_label.setText("ìë™ ì¸ì‹ ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
            self._set_check_type("in")
            if hasattr(self, "check_type_container"):
                self.check_type_container.setVisible(True)
            # ìë™ ìŒì„± ì¸ì‹ ì‹œì‘
            self.voice_auto_timer.start(self.voice_auto_interval_ms)
            
        elif mode_name == "attendance_status":
            self.update_status("ğŸ“Š ì¶œì„ í˜„í™© ì¡°íšŒ")
            self._show_plot_view()
            self.show_attendance_status_plot(date=datetime.now().date())
        else:
            if hasattr(self, "check_type_container"):
                self.check_type_container.setVisible(False)
    
    def update_status(self, message):
        """ìƒíƒœë°” ì—…ë°ì´íŠ¸"""
        self.status_bar.setText(message)

    def _show_camera_view(self):
        if hasattr(self, "camera_stack"):
            self.camera_stack.setCurrentWidget(self.camera_label)
        if hasattr(self, "attendance_toolbar"):
            self.attendance_toolbar.setVisible(False)
        if hasattr(self, "check_type_container"):
            self.check_type_container.setVisible(False)

    def _show_plot_view(self):
        if hasattr(self, "camera_stack"):
            self.camera_stack.setCurrentWidget(self.plot_container)
        if hasattr(self, "attendance_toolbar"):
            self.attendance_toolbar.setVisible(True)
        if hasattr(self, "check_type_container"):
            self.check_type_container.setVisible(False)

    def _set_check_type(self, mode):
        self.attendance_check_type = mode
        if mode == "in":
            self.check_in_btn.setStyleSheet(self.get_button_style(font_size=11) + f"border: 2px solid {SUCCESS_COLOR};")
            self.check_out_btn.setStyleSheet(self.get_button_style(font_size=11))
        else:
            self.check_out_btn.setStyleSheet(self.get_button_style(font_size=11) + f"border: 2px solid {WARNING_COLOR};")
            self.check_in_btn.setStyleSheet(self.get_button_style(font_size=11))

    def _on_today_attendance_clicked(self):
        self.show_attendance_status_plot(date=datetime.now().date())

    def _on_date_attendance_clicked(self):
        date_str, ok = QInputDialog.getText(self, "ë‚ ì§œë³„ ì¡°íšŒ", "ë‚ ì§œë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYY-MM-DD):")
        if not ok or not date_str.strip():
            return
        try:
            selected_date = datetime.strptime(date_str.strip(), "%Y-%m-%d").date()
        except ValueError:
            QMessageBox.warning(self, "ì¶œì„ í˜„í™©", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        student_name, ok = QInputDialog.getText(self, "í•™ìƒ ì§€ì •", "í•™ìƒ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”:")
        if not ok or not student_name.strip():
            QMessageBox.warning(self, "ì¶œì„ í˜„í™©", "í•™ìƒ ì´ë¦„ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            return

        self.show_attendance_status_plot(date=selected_date, student_name=student_name.strip())

    def _on_range_attendance_clicked(self):
        start_str, ok = QInputDialog.getText(self, "ê¸°ê°„ ì¡°íšŒ", "ì‹œì‘ ë‚ ì§œë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYY-MM-DD):")
        if not ok or not start_str.strip():
            return
        end_str, ok = QInputDialog.getText(self, "ê¸°ê°„ ì¡°íšŒ", "ì¢…ë£Œ ë‚ ì§œë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYY-MM-DD):")
        if not ok or not end_str.strip():
            return

        try:
            start_date = datetime.strptime(start_str.strip(), "%Y-%m-%d").date()
            end_date = datetime.strptime(end_str.strip(), "%Y-%m-%d").date()
        except ValueError:
            QMessageBox.warning(self, "ì¶œì„ í˜„í™©", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        if end_date < start_date:
            QMessageBox.warning(self, "ì¶œì„ í˜„í™©", "ì¢…ë£Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œë³´ë‹¤ ë¹ ë¥¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        student_name, ok = QInputDialog.getText(self, "í•™ìƒ ì§€ì •", "í•™ìƒ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ):")
        if not ok:
            return

        student_name = student_name.strip() if student_name else None
        self.show_attendance_status_plot(start_date=start_date, end_date=end_date, student_name=student_name)

    def _get_attendance_db_config(self):
        return {
            "host": os.getenv("DB_HOST", "192.168.0.41"),
            "port": int(os.getenv("DB_PORT", "5432")),
            "dbname": os.getenv("DB_NAME", "devserver"),
            "user": os.getenv("DB_USER", "orugu"),
            "password": os.getenv("DB_PASSWORD", "orugu#0916"),
        }

    def _fetch_attendance_records(self, limit=200, date_filter=None, start_date=None, end_date=None, employee_no=None, student_name=None):
        cfg = self._get_attendance_db_config()
        records = []
        sql = (
            'SELECT id, employee_no, name, email, created_at, check_in_time, check_out_time '
            'FROM "UserData"."userdata"'
        )
        conditions = []
        params = []

        if date_filter:
            conditions.append('DATE(check_in_time) = %s')
            params.append(date_filter)
        if start_date and end_date:
            conditions.append('DATE(check_in_time) BETWEEN %s AND %s')
            params.extend([start_date, end_date])
        if employee_no:
            conditions.append('employee_no = %s')
            params.append(employee_no)
        if student_name:
            conditions.append('name = %s')
            params.append(student_name)

        if conditions:
            sql += ' WHERE ' + ' AND '.join(conditions)

        sql += ' ORDER BY id DESC LIMIT %s'
        params.append(limit)

        encoding_primary = os.getenv("ATTENDANCE_DB_ENCODING", "UTF8")
        encoding_fallback = os.getenv("ATTENDANCE_DB_ENCODING_FALLBACK", "EUC_KR")
        encoding_last_resort = os.getenv("ATTENDANCE_DB_ENCODING_LAST", "LATIN1")

        def _run_query(client_encoding: str):
            os.environ["PGCLIENTENCODING"] = client_encoding
            import psycopg2
            with psycopg2.connect(
                host=cfg["host"],
                port=cfg["port"],
                dbname=cfg["dbname"],
                user=cfg["user"],
                password=cfg["password"],
                connect_timeout=5,
                options=f"-c client_encoding={client_encoding} -c lc_messages=C"
            ) as conn:
                conn.set_client_encoding(client_encoding)
                with conn.cursor() as cur:
                    cur.execute(sql, params)
                    return cur.fetchall()

        try:
            rows = _run_query(encoding_primary)
        except UnicodeDecodeError:
            try:
                rows = _run_query(encoding_fallback)
            except UnicodeDecodeError:
                rows = _run_query(encoding_last_resort)

        for row in rows:
            records.append({
                "id": row[0],
                "employee_no": row[1],
                "name": row[2],
                "email": row[3],
                "created_at": row[4],
                "check_in_time": row[5],
                "check_out_time": row[6],
            })
        return records

    def _ensure_userdata_table_schema(self, conn):
        """userdata í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ë° ìë™ ìƒì„±/ìˆ˜ì •"""
        try:
            with conn.cursor() as cur:
                # 1. ìŠ¤í‚¤ë§ˆ ì¡´ì¬ í™•ì¸
                cur.execute("""
                    SELECT schema_name FROM information_schema.schemata 
                    WHERE schema_name = 'UserData'
                """)
                if not cur.fetchone():
                    print("âœ“ UserData ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘...")
                    cur.execute('CREATE SCHEMA "UserData"')
                    conn.commit()

                # 2. í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                cur.execute("""
                    SELECT table_name FROM information_schema.tables 
                    WHERE table_schema = 'UserData' AND table_name = 'userdata'
                """)
                table_exists = cur.fetchone() is not None

                if not table_exists:
                    print("âœ“ userdata í…Œì´ë¸” ìƒì„± ì¤‘...")
                    cur.execute("""
                        CREATE TABLE "UserData"."userdata" (
                            id SERIAL PRIMARY KEY,
                            employee_no VARCHAR NOT NULL,
                            name VARCHAR NOT NULL,
                            email VARCHAR NOT NULL,
                            created_at TIMESTAMP NOT NULL,
                            check_in_time TIMESTAMP,
                            check_out_time TIMESTAMP
                        )
                    """)
                    conn.commit()
                    print("âœ“ userdata í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
                    return

                # 3. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
                cur.execute("""
                    SELECT column_name FROM information_schema.columns
                    WHERE table_schema = 'UserData' AND table_name = 'userdata'
                """)
                existing_columns = {row[0] for row in cur.fetchall()}

                required_columns = {
                    'id': 'SERIAL PRIMARY KEY',
                    'employee_no': 'VARCHAR NOT NULL',
                    'name': 'VARCHAR NOT NULL',
                    'email': 'VARCHAR NOT NULL',
                    'created_at': 'TIMESTAMP NOT NULL',
                    'check_in_time': 'TIMESTAMP',
                    'check_out_time': 'TIMESTAMP'
                }

                for col_name, col_type in required_columns.items():
                    if col_name not in existing_columns:
                        print(f"âœ“ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘: {col_name}")
                        if col_name == 'id':
                            # id ì»¬ëŸ¼ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                            cur.execute(f'''
                                ALTER TABLE "UserData"."userdata" 
                                ADD COLUMN {col_name} SERIAL PRIMARY KEY
                            ''')
                        else:
                            # ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤
                            if 'NOT NULL' in col_type:
                                # NOT NULL ì»¬ëŸ¼ì€ ê¸°ë³¸ê°’ ì œê³µ
                                default_val = "''" if 'VARCHAR' in col_type else 'NOW()'
                                cur.execute(f'''
                                    ALTER TABLE "UserData"."userdata" 
                                    ADD COLUMN {col_name} {col_type.replace('NOT NULL', '')} DEFAULT {default_val}
                                ''')
                                cur.execute(f'''
                                    ALTER TABLE "UserData"."userdata" 
                                    ALTER COLUMN {col_name} SET NOT NULL
                                ''')
                            else:
                                cur.execute(f'''
                                    ALTER TABLE "UserData"."userdata" 
                                    ADD COLUMN {col_name} {col_type}
                                ''')
                        conn.commit()
                        print(f"âœ“ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {col_name}")

        except Exception as e:
            print(f"âš ï¸  í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            conn.rollback()

    def _mark_attendance_if_needed(self, name, fusion_score):
        if not name or name == "Unknown":
            return
        now = datetime.now()
        cfg = self._get_attendance_db_config()

        encoding_primary = os.getenv("ATTENDANCE_DB_ENCODING", "UTF8")
        encoding_fallback = os.getenv("ATTENDANCE_DB_ENCODING_FALLBACK", "EUC_KR")
        encoding_last_resort = os.getenv("ATTENDANCE_DB_ENCODING_LAST", "LATIN1")

        def _run(client_encoding: str):
            os.environ["PGCLIENTENCODING"] = client_encoding
            import psycopg2
            with psycopg2.connect(
                host=cfg["host"],
                port=cfg["port"],
                dbname=cfg["dbname"],
                user=cfg["user"],
                password=cfg["password"],
                connect_timeout=5,
                options=f"-c client_encoding={client_encoding} -c lc_messages=C"
            ) as conn:
                conn.set_client_encoding(client_encoding)
                
                # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ë° ìë™ ìƒì„±
                self._ensure_userdata_table_schema(conn)
                
                with conn.cursor() as cur:
                    cur.execute(
                        'SELECT user_id, email FROM "UserData".userbasicdata WHERE name = %s ORDER BY user_id LIMIT 1',
                        (name,)
                    )
                    row = cur.fetchone()
                    employee_no = row[0] if row else "UNKNOWN"
                    email = row[1] if row else "unknown@example.com"

                    cur.execute(
                        'SELECT id FROM "UserData"."userdata" WHERE employee_no = %s AND DATE(check_in_time) = %s LIMIT 1',
                        (employee_no, now.date())
                    )
                    existing = cur.fetchone()

                    if self.attendance_check_type == "out":
                        if existing:
                            cur.execute(
                                'UPDATE "UserData"."userdata" SET check_out_time = %s WHERE id = %s',
                                (now, existing[0])
                            )
                        else:
                            cur.execute(
                                'INSERT INTO "UserData"."userdata" (employee_no, name, email, created_at, check_in_time, check_out_time) '
                                'VALUES (%s, %s, %s, %s, %s, %s)',
                                (employee_no, name, email, now, now, now)
                            )
                        return

                    if existing:
                        return

                    cur.execute(
                        'INSERT INTO "UserData"."userdata" (employee_no, name, email, created_at, check_in_time) '
                        'VALUES (%s, %s, %s, %s, %s)',
                        (employee_no, name, email, now, now)
                    )
                conn.commit()

        try:
            _run(encoding_primary)
        except UnicodeDecodeError:
            try:
                _run(encoding_fallback)
            except UnicodeDecodeError:
                _run(encoding_last_resort)

    def show_attendance_status_plot(self, date=None, start_date=None, end_date=None, employee_no=None, student_name=None):
        # Matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¸°ë³¸: ë§‘ì€ ê³ ë”•)
        try:
            malgun = "C:\\Windows\\Fonts\\malgun.ttf"
            if os.path.exists(malgun):
                font_manager.fontManager.addfont(malgun)
                rcParams["font.family"] = "Malgun Gothic"
            else:
                for name in ["NanumGothic", "AppleGothic", "Malgun Gothic"]:
                    if any(f.name == name for f in font_manager.fontManager.ttflist):
                        rcParams["font.family"] = name
                        break
            rcParams["axes.unicode_minus"] = False
        except Exception:
            pass

        try:
            records = self._fetch_attendance_records(
                limit=200,
                date_filter=date,
                start_date=start_date,
                end_date=end_date,
                employee_no=employee_no,
                student_name=student_name
            )
        except UnicodeDecodeError:
            cfg = self._get_attendance_db_config()
            QMessageBox.warning(
                self,
                "ì¶œì„ í˜„í™©",
                "DB ì—°ê²° ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
                "DB_USER/DB_PASSWORD ë˜ëŠ” DB_HOST/DB_NAMEì´ ë‹¤ë¥¸ ì„œë²„ì™€ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"í˜„ì¬ ì„¤ì •: {cfg['host']}:{cfg['port']} / {cfg['dbname']} / {cfg['user']}"
            )
            return
        except Exception as e:
            QMessageBox.warning(self, "ì¶œì„ í˜„í™©", f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return

        if not records:
            self.plot_figure.clear()
            ax = self.plot_figure.add_subplot(111)
            ax.axis('off')
            ax.text(0.5, 0.5, "ìë£Œê°€ ì—†ì–´ìš” ã… .ã… ", ha='center', va='center', fontsize=14)
            self.plot_canvas.draw()
            return

        records = list(reversed(records))
        labels = [f"{r['employee_no']} {r['name']}" for r in records]

        def dt_to_minutes(dt_value):
            if not isinstance(dt_value, datetime):
                return None
            return dt_value.hour * 60 + dt_value.minute + dt_value.second / 60.0

        check_in = [dt_to_minutes(r["check_in_time"]) for r in records]
        check_out = [dt_to_minutes(r["check_out_time"]) for r in records]

        x = list(range(len(labels)))

        self.plot_figure.clear()
        ax = self.plot_figure.add_subplot(111)
        ax.plot(x, check_in, marker='o', label='Check-in')
        ax.plot(x, check_out, marker='o', label='Check-out')

        title = "ì¶œì„ í˜„í™© (Check-in/Check-out)"
        if start_date and end_date:
            if employee_no or student_name:
                target = employee_no or student_name
                title = f"ì¶œì„ í˜„í™© - {target} ({start_date}~{end_date})"
            else:
                title = f"ê¸°ê°„ ì¶œì„ í˜„í™© ({start_date}~{end_date})"
        elif date and (employee_no or student_name):
            target = employee_no or student_name
            title = f"ì¶œì„ í˜„í™© - {target} ({date})"
        elif date:
            title = f"ê¸ˆì¼ ì¶œì„ í˜„í™© ({date})"
        ax.set_title(title)
        ax.set_xlabel("ì‚¬ìš©ì")
        ax.set_ylabel("ì‹œê°„")
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        ax.legend()

        def minutes_to_hhmm(value, _):
            if value is None:
                return ""
            hours = int(value // 60)
            minutes = int(value % 60)
            return f"{hours:02d}:{minutes:02d}"

        ax.yaxis.set_major_formatter(ticker.FuncFormatter(minutes_to_hhmm))
        self.plot_figure.tight_layout()
        self.plot_canvas.draw()
    
    def keyPressEvent(self, event):
        """í‚¤ë³´ë“œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
        super().keyPressEvent(event)

    def _start_voice_auto_recognition(self):
        """ìë™ ëª¨ë“œì—ì„œ ì£¼ê¸°ì  ìŒì„± ì¸ì‹ ì‹¤í–‰"""
        if self.current_mode != "automation":
            if self.voice_auto_timer.isActive():
                self.voice_auto_timer.stop()
            return

        if self.voice_auto_running:
            return

        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            self.update_status("âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return

        self.voice_auto_running = True
        threading.Thread(target=self._voice_auto_worker, daemon=True).start()

    def _start_voice_live_recognition(self):
        """ìŒì„± ëª¨ë“œì—ì„œ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹¤í–‰"""
        if self.current_mode != "voice_attendance":
            if self.voice_live_timer.isActive():
                self.voice_live_timer.stop()
            return

        if self.voice_live_running:
            return

        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            self.update_status("âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return

        self.voice_live_running = True
        threading.Thread(target=self._voice_live_worker, daemon=True).start()

    def _voice_auto_worker(self):
        try:
            name, confidence, error = self.record_voice_and_recognize_internal(
                duration=3,
                sample_rate=16000,
                ui_updates=False
            )
            if error:
                return

            if name != "Unknown":
                self.last_voice_result = (name, confidence)
                self.voice_result_time = time.time()
        finally:
            self.voice_auto_running = False

    def _voice_live_worker(self):
        try:
            name, confidence, error = self.record_voice_and_recognize_internal(
                duration=self.voice_live_duration,
                sample_rate=16000,
                ui_updates=False
            )
            if error == "no_voice":
                return
            self.voice_attendance_result.emit(name, confidence, error)
        finally:
            self.voice_live_running = False

    def _voice_attendance_worker(self):
        name, confidence, error = self.record_voice_and_recognize_internal(
            duration=3,
            sample_rate=16000,
            ui_updates=False
        )
        self.voice_attendance_result.emit(name, confidence, error)

    def handle_voice_attendance_result(self, name, confidence, error):
        threshold = self.voice_similarity_threshold

        if error == "no_voice":
            return

        if error:
            self.attendance_status_label.setText("ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
            self.attendance_status_label.setStyleSheet(
                f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;"
            )
            self.update_status(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {error}")
            return

        print(f"ğŸ” ìŒì„± ì¸ì‹ ê²°ê³¼: {name} (ìœ ì‚¬ë„: {confidence:.3f}, ì„ê³„ê°’: {threshold})")
        print(f"ğŸ” ë“±ë¡ëœ ìŒì„± ìˆ˜: {len(self.known_voice_names)}")

        if name != "Unknown" and confidence >= threshold:
            self.user_name_label.setText(f"ì´ë¦„: {name}")
            self.attendance_status_label.setText(f"ìŒì„± ì¸ì‹ë¨ ({confidence:.2f})")
            self.attendance_status_label.setStyleSheet(
                f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
            )
            self.update_status(f"âœ… {name} ìŒì„± ì¸ì‹ ì„±ê³µ")
            self.process_voice_event(name, confidence)

            QMessageBox.information(
                self,
                "ìŒì„±ì¶œì„ ì™„ë£Œ",
                "ìŒì„±ì¶œì„ ì™„ë£Œ!"
            )
        else:
            self.attendance_status_label.setText("ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
            self.attendance_status_label.setStyleSheet(
                f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;"
            )
            self.update_status(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨ (ìœ ì‚¬ë„: {confidence:.2f}, ì„ê³„ê°’: {threshold})")
    
    def closeEvent(self, event):
        """ìœˆë„ìš° ì¢…ë£Œ ì´ë²¤íŠ¸"""
        if self.camera is not None:
            self.camera.release()
        if MEDIAPIPE_AVAILABLE:
            if hasattr(self, 'face_detector') and self.face_detector:
                self.face_detector.close()
            if hasattr(self, 'gesture_recognizer') and self.gesture_recognizer:
                self.gesture_recognizer.close()
        event.accept()


def main():
    app = QApplication(sys.argv)
    
    # ë‹¤í¬ í…Œë§ˆ ì ìš©
    app.setStyle('Fusion')
    palette = QPalette()
    palette.setColor(QPalette.Window, QColor(30, 39, 46))
    palette.setColor(QPalette.WindowText, Qt.white)
    app.setPalette(palette)
    
    window = ClientUI()
    window.show()
    
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()
