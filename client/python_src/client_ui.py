"""
í´ë¼ì´ì–¸íŠ¸ UI - ì¶œê²°ê´€ë¦¬ ì‹œìŠ¤í…œ
ì œìŠ¤ì²˜ ë° ì–¼êµ´ ì¸ì‹ ì¶œì„ ì²´í¬
"""

import sys
import cv2
import os
import pickle
import time
import numpy as np
from dotenv import load_dotenv
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QLabel, QPushButton,
    QVBoxLayout, QHBoxLayout, QMessageBox, QFrame, QFileDialog
)
from PyQt5.QtCore import Qt, QTimer
from PyQt5.QtGui import QImage, QPixmap, QFont, QPalette, QColor

load_dotenv()

# MediaPipe import
MEDIAPIPE_AVAILABLE = False
try:
    import mediapipe as mp
    # Task API import ì‹œë„
    try:
        from mediapipe.tasks import python
        from mediapipe.tasks.python import vision
        from mediapipe import Image as MPImage
        MEDIAPIPE_AVAILABLE = True
        USE_TASK_API = True
        print("âœ“ MediaPipe Task API ì‚¬ìš© ê°€ëŠ¥")
    except:
        # Task API ì—†ìœ¼ë©´ OpenCVë§Œ ì‚¬ìš©
        USE_TASK_API = False
        MEDIAPIPE_AVAILABLE = False
        print("â„¹ï¸  MediaPipe Task APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenCVë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸  MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# SpeechBrain ìŒì„± ì¸ì‹ ëª¨ë¸ (ì˜µì…˜)
SPEECHBRAIN_AVAILABLE = False
try:
    import torchaudio
    from speechbrain.inference.speaker import EncoderClassifier
    SPEECHBRAIN_AVAILABLE = True
    print("âœ“ SpeechBrain ì‚¬ìš© ê°€ëŠ¥")
except Exception:
    SPEECHBRAIN_AVAILABLE = False
    print("â„¹ï¸  SpeechBrain(ìŒì„± ëª¨ë¸)ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# UI ì„¤ì • ì„í¬íŠ¸
from lib.ui_config_lib import *


class ClientUI(QMainWindow):
    """í´ë¼ì´ì–¸íŠ¸ ì¶œì„ ì²´í¬ ìœˆë„ìš°"""
    
    def __init__(self):
        super().__init__()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        self.camera = None
        self.current_frame = None
        self.current_mode = None  # 'gesture', 'face', 'voice', None
        self.current_user = None
        
        # ì–¼êµ´ ê°ì§€ê¸° ì´ˆê¸°í™”
        self.face_detector = None
        self.gesture_recognizer = None
        
        # ì–¼êµ´ ì¸ì‹ ì„¤ì •
        self.face_confidence_threshold = float(os.getenv('CONFIDENCE_THRESHOLD', 0.70))
        self.face_similarity_threshold = float(os.getenv('FACE_SIMILARITY_THRESHOLD', 0.70))
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì •
        self.gesture_confidence_threshold = float(os.getenv('GESTURE_CONFIDENCE_THRESHOLD', 0.5))
        self.gesture_cooldown = float(os.getenv('GESTURE_COOLDOWN', 3.0))
        self.last_gesture_time = {}  # {gesture_type: timestamp}
        self.detected_gestures = []  # ê°ì§€ëœ ì œìŠ¤ì²˜ íˆìŠ¤í† ë¦¬
        
        # ìŒì„± ì¸ì‹ ì„¤ì •
        self.voice_encoder = None
        self.known_voice_embeddings = []
        self.known_voice_names = []
        self.voice_similarity_threshold = float(os.getenv('VOICE_SIMILARITY_THRESHOLD', 0.7))
        self.voice_model_path = os.getenv('VOICE_MODEL_PATH', 'models/spkrec-ecapa-voxceleb')
        self.last_voice_result = None  # ë§ˆì§€ë§‰ ìŒì„± ì¸ì‹ ê²°ê³¼ (name, confidence)
        self.voice_result_time = 0  # ë§ˆì§€ë§‰ ìŒì„± ì¸ì‹ ì‹œê°„
        
        if MEDIAPIPE_AVAILABLE and USE_TASK_API:
            try:
                # ì–¼êµ´ ê°ì§€ê¸°
                base_options_face = python.BaseOptions(model_asset_path='models/blaze_face_short_range.tflite')
                face_options = vision.FaceDetectorOptions(base_options=base_options_face)
                self.face_detector = vision.FaceDetector.create_from_options(face_options)
                
                # ì œìŠ¤ì²˜ ì¸ì‹ê¸°
                base_options_gesture = python.BaseOptions(model_asset_path='models/gesture_recognizer.task')
                gesture_options = vision.GestureRecognizerOptions(base_options=base_options_gesture)
                self.gesture_recognizer = vision.GestureRecognizer.create_from_options(gesture_options)
                
                print("âœ“ MediaPipe Task API ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("â„¹ï¸  ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”: models/blaze_face_short_range.tflite, models/gesture_recognizer.task")
        
        # ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”
        if SPEECHBRAIN_AVAILABLE:
            try:
                # ëª¨ë¸ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤
                self.voice_encoder = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir=self.voice_model_path
                )
                self.load_voice_data()
                print("âœ“ ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        
        # ì–¼êµ´ì¸ì‹ ë°ì´í„° ë¡œë“œ
        self.known_face_features = []
        self.known_face_names = []
        self.load_face_data()
        
        # UI ì´ˆê¸°í™”
        self.init_ui()
        
        # ì¹´ë©”ë¼ ì‹œì‘
        self.start_camera()
        
    def init_ui(self):
        """UI ì´ˆê¸°í™”"""
        # ìœˆë„ìš° ì„¤ì •
        self.setWindowTitle(WINDOW_TITLE)
        self.setGeometry(100, 100, WINDOW_WIDTH, WINDOW_HEIGHT)
        self.setStyleSheet(f"background-color: {BG_COLOR};")
        
        # ì¤‘ì•™ ìœ„ì ¯
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ (ìˆ˜í‰)
        main_layout = QHBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±
        sidebar = self.create_sidebar()
        main_layout.addWidget(sidebar)
        
        # ì¤‘ì•™ + ìš°ì¸¡ ì˜ì—­
        center_right_layout = QVBoxLayout()
        center_right_layout.setContentsMargins(20, 20, 20, 20)
        
        # ì¤‘ì•™(ì¹´ë©”ë¼) + ìš°ì¸¡(ì •ë³´) ì˜ì—­
        cam_info_layout = QHBoxLayout()
        
        # ì¹´ë©”ë¼ ì˜ì—­
        self.camera_label = self.create_camera_view()
        cam_info_layout.addWidget(self.camera_label)
        
        # ìš°ì¸¡ ì •ë³´ íŒ¨ë„
        right_panel = self.create_right_panel()
        cam_info_layout.addWidget(right_panel)
        
        center_right_layout.addLayout(cam_info_layout)
        
        # í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ
        gesture_guide = self.create_gesture_guide()
        center_right_layout.addWidget(gesture_guide)
        
        # í•˜ë‹¨ ìƒíƒœë°”
        status_bar = self.create_status_bar()
        center_right_layout.addWidget(status_bar)
        
        main_layout.addLayout(center_right_layout)
        
    def create_sidebar(self):
        """ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±"""
        sidebar = QWidget()
        sidebar.setFixedWidth(SIDEBAR_WIDTH)
        sidebar.setStyleSheet(f"background-color: {SIDEBAR_COLOR};")
        
        layout = QVBoxLayout(sidebar)
        layout.setContentsMargins(SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING)
        layout.setSpacing(0)
        
        # í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ ë¼ë²¨
        client_label = QLabel("ğŸ“± ì¶œì„ ì²´í¬")
        client_label.setFixedHeight(CLIENT_LABEL_HEIGHT)
        client_label.setAlignment(Qt.AlignCenter)
        client_label.setStyleSheet(f"""
            color: {TEXT_COLOR};
            font-size: {CLIENT_LABEL_FONT_SIZE}px;
            font-weight: {CLIENT_LABEL_FONT_WEIGHT};
            background-color: {ACCENT_COLOR};
            border-radius: 5px;
            padding: 10px;
        """)
        layout.addWidget(client_label)
        
        layout.addSpacing(LEFT_BUTTON_START_Y - CLIENT_LABEL_HEIGHT - SIDEBAR_PADDING)
        
        # ì¢Œì¸¡ ë²„íŠ¼ë“¤ ìƒì„±
        self.left_buttons = {}
        for btn_config in LEFT_BUTTONS:
            btn = QPushButton(btn_config["text"])
            btn.setFixedSize(LEFT_BUTTON_WIDTH, LEFT_BUTTON_HEIGHT)
            btn.setStyleSheet(self.get_button_style())
            btn.setCursor(Qt.PointingHandCursor)
            
            # ë²„íŠ¼ ì´ë²¤íŠ¸ ì—°ê²°
            btn_name = btn_config["name"]
            btn.clicked.connect(lambda checked, name=btn_name: self.on_mode_button_click(name))
            
            self.left_buttons[btn_name] = btn
            layout.addWidget(btn)
            layout.addSpacing(LEFT_BUTTON_SPACING)
        
        layout.addStretch()
        
        return sidebar
    
    def create_camera_view(self):
        """ì¹´ë©”ë¼ ë·° ìƒì„±"""
        camera_label = QLabel()
        camera_label.setFixedSize(CAM_WIDTH, CAM_HEIGHT)
        camera_label.setAlignment(Qt.AlignCenter)
        camera_label.setStyleSheet(f"""
            background-color: {CAM_BG_COLOR};
            border: 3px solid {ACCENT_COLOR};
            border-radius: 10px;
        """)
        camera_label.setText("ğŸ“¹ ì¹´ë©”ë¼ ë¡œë”© ì¤‘...")
        camera_label.setFont(QFont("Arial", 14))
        camera_label.setStyleSheet(camera_label.styleSheet() + f"color: {TEXT_COLOR};")
        
        return camera_label
    
    def create_right_panel(self):
        """ìš°ì¸¡ ì •ë³´ íŒ¨ë„ ìƒì„±"""
        panel = QWidget()
        panel.setFixedWidth(RIGHT_PANEL_WIDTH)
        
        layout = QVBoxLayout(panel)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(15)
        
        # ì‚¬ìš©ì ì •ë³´ í”„ë ˆì„
        user_info_frame = QFrame()
        user_info_frame.setFixedHeight(USER_INFO_HEIGHT)
        user_info_frame.setStyleSheet(f"""
            background-color: {USER_INFO_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        user_info_layout = QVBoxLayout(user_info_frame)
        
        user_title = QLabel("ğŸ‘¤ ì‚¬ìš©ì ì •ë³´")
        user_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        user_info_layout.addWidget(user_title)
        
        self.user_name_label = QLabel("ì´ë¦„: -")
        self.user_name_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_name_label)
        
        self.user_id_label = QLabel("í•™ë²ˆ: -")
        self.user_id_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_id_label)
        
        user_info_layout.addStretch()
        
        layout.addWidget(user_info_frame)
        
        # ì¶œì„ ìƒíƒœ í”„ë ˆì„
        status_frame = QFrame()
        status_frame.setFixedHeight(ATTENDANCE_STATUS_HEIGHT)
        status_frame.setStyleSheet(f"""
            background-color: {ATTENDANCE_STATUS_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        status_layout = QVBoxLayout(status_frame)
        
        status_title = QLabel("ğŸ“Š ì¶œì„ ìƒíƒœ")
        status_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        status_layout.addWidget(status_title)
        
        self.attendance_status_label = QLabel("ëŒ€ê¸° ì¤‘...")
        self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
        self.attendance_status_label.setAlignment(Qt.AlignCenter)
        status_layout.addWidget(self.attendance_status_label)
        
        self.detected_gesture_label = QLabel("ì œìŠ¤ì²˜: -")
        self.detected_gesture_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        status_layout.addWidget(self.detected_gesture_label)
        
        status_layout.addStretch()
        
        layout.addWidget(status_frame)
        
        layout.addStretch()
        
        return panel
    
    def create_gesture_guide(self):
        """í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ ìƒì„±"""
        guide_frame = QFrame()
        guide_frame.setFixedHeight(GESTURE_GUIDE_HEIGHT)
        guide_frame.setStyleSheet(f"""
            background-color: {GESTURE_GUIDE_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        layout = QVBoxLayout(guide_frame)
        
        title = QLabel("ğŸ‘‹ ì œìŠ¤ì²˜ ê°€ì´ë“œ")
        title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 13px; font-weight: bold;")
        layout.addWidget(title)
        
        # ì œìŠ¤ì²˜ ëª©ë¡
        gestures_layout = QHBoxLayout()
        
        for gesture_name, gesture_info in GESTURES.items():
            gesture_widget = QLabel(f"{gesture_info['emoji']}\n{gesture_info['text']}")
            gesture_widget.setAlignment(Qt.AlignCenter)
            gesture_widget.setStyleSheet(f"""
                color: {gesture_info['color']};
                font-size: 11px;
                padding: 5px;
            """)
            gestures_layout.addWidget(gesture_widget)
        
        layout.addLayout(gestures_layout)
        
        return guide_frame
    
    def create_status_bar(self):
        """í•˜ë‹¨ ìƒíƒœë°” ìƒì„±"""
        status_bar = QLabel("âœ… ì¤€ë¹„ ì™„ë£Œ - ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
        status_bar.setFixedHeight(STATUS_BAR_HEIGHT)
        status_bar.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)
        status_bar.setStyleSheet(f"""
            background-color: {STATUS_BAR_BG_COLOR};
            color: {TEXT_COLOR};
            font-size: {STATUS_FONT_SIZE}px;
            padding-left: 15px;
            border-radius: 5px;
        """)
        self.status_bar = status_bar
        return status_bar
    
    def get_button_style(self, font_size=LEFT_BUTTON_FONT_SIZE):
        """ë²„íŠ¼ ìŠ¤íƒ€ì¼ ë°˜í™˜"""
        return f"""
            QPushButton {{
                background-color: {BUTTON_COLOR};
                color: {TEXT_COLOR};
                border: none;
                border-radius: 5px;
                font-size: {font_size}px;
                font-weight: bold;
                padding: 5px;
            }}
            QPushButton:hover {{
                background-color: {BUTTON_HOVER_COLOR};
            }}
            QPushButton:pressed {{
                background-color: #1B1464;
            }}
        """
    
    def start_camera(self):
        """ì¹´ë©”ë¼ ì‹œì‘"""
        self.camera = cv2.VideoCapture(CAMERA_INDEX)
        
        if not self.camera.isOpened():
            self.update_status("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì¹´ë©”ë¼ í•´ìƒë„ ì„¤ì •
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, CAM_WIDTH)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, CAM_HEIGHT)
        
        # íƒ€ì´ë¨¸ë¡œ í”„ë ˆì„ ì—…ë°ì´íŠ¸
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(int(1000 / CAMERA_FPS))
        
        self.update_status("ğŸ“¹ ì¹´ë©”ë¼ í™œì„±í™”ë¨")
    
    def load_voice_data(self):
        """ì €ì¥ëœ ìŒì„±(ì„ë² ë”©) ë°ì´í„° ë¡œë“œ"""
        voice_data_file = "../data/voice/voice_embeddings.pkl"

        if os.path.exists(voice_data_file):
            try:
                with open(voice_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_voice_embeddings = data.get('embeddings', [])
                    self.known_voice_names = data.get('names', [])
                print(f"âœ“ {len(self.known_voice_names)}ëª…ì˜ ìŒì„± ë°ì´í„° ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("â„¹ï¸  ë“±ë¡ëœ ìŒì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def save_voice_data(self):
        """ìŒì„± ì„ë² ë”© ì €ì¥"""
        voice_data_file = "../data/voice/voice_embeddings.pkl"
        os.makedirs(os.path.dirname(voice_data_file), exist_ok=True)

        data = {
            'embeddings': self.known_voice_embeddings,
            'names': self.known_voice_names
        }

        try:
            with open(voice_data_file, 'wb') as f:
                pickle.dump(data, f)
            print("âœ“ ìŒì„± ë°ì´í„° ì €ì¥ë¨")
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def extract_voice_embedding(self, audio_file):
        """ìŒì„± íŒŒì¼ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            signal, sr = torchaudio.load(audio_file)
            emb = self.voice_encoder.encode_batch(signal)
            return emb.detach().cpu().numpy()
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def recognize_voice(self, audio_file):
        """ìŒì„± íŒŒì¼ ì¸ì‹"""
        embedding = self.extract_voice_embedding(audio_file)
        
        if embedding is None:
            return "Unknown", 0.0
        
        best_match_name = "Unknown"
        best_similarity = 0.0
        
        # ì €ì¥ëœ ìŒì„±ê³¼ ë¹„êµ
        for known_emb, known_name in zip(self.known_voice_embeddings, self.known_voice_names):
            similarity = self.cosine_similarity(embedding.flatten(), np.array(known_emb))
            if similarity > best_similarity:
                best_similarity = float(similarity)
                best_match_name = known_name
        
        # ì„ê³„ê°’ í™•ì¸
        if best_similarity < self.voice_similarity_threshold:
            best_match_name = "Unknown"
            best_similarity = 0.0
        
        return best_match_name, best_similarity
    
    def record_voice_and_extract(self, filename="./tmp_voice.wav", duration=3, fs=16000):
        """ìŒì„± ë…¹ìŒ ë° ì„ë² ë”© ì¶”ì¶œ (placeholder)"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        try:
            signal, sr = torchaudio.load(filename)
            emb = self.voice_encoder.encode_batch(signal)
            return emb.detach().cpu().numpy()
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def process_voice_event(self, name, confidence):
        """ê°ì§€ëœ ìŒì„± ì²˜ë¦¬"""
        self.user_name_label.setText(f"ì´ë¦„: {name}")
        self.attendance_status_label.setText(f"ìŒì„± ì¸ì‹ë¨ ({confidence:.1%})")
        self.attendance_status_label.setStyleSheet(
            f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
        )
        print(f"ğŸ“¤ ìŒì„± ì´ë²¤íŠ¸: {name} ({confidence:.1%})")
    
    def load_face_data(self):
        """ì €ì¥ëœ ì–¼êµ´ ë°ì´í„° ë¡œë“œ"""
        face_data_file = "../data/face_data.pkl"
        
        if os.path.exists(face_data_file):
            try:
                with open(face_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_face_features = data.get('features', [])
                    self.known_face_names = data.get('names', [])
                print(f"âœ“ {len(self.known_face_names)}ëª…ì˜ ì–¼êµ´ ë°ì´í„° ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ì–¼êµ´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("â„¹ï¸  ë“±ë¡ëœ ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def extract_face_features(self, detection, image_width, image_height):
        """ì–¼êµ´ ê°ì§€ ê²°ê³¼ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
        bbox = detection.bounding_box
        features = [
            bbox.origin_x / image_width,
            bbox.origin_y / image_height,
            bbox.width / image_width,
            bbox.height / image_height
        ]
        return np.array(features)
    
    def cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    def recognize_faces(self, frame):
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ì¸ì‹ (MediaPipe Task API ê¸°ë°˜)"""
        if not self.face_detector:
            # MediaPipeê°€ ì—†ìœ¼ë©´ OpenCV Haar Cascade ì‚¬ìš©
            return self.recognize_faces_opencv(frame)
        
        # MediaPipe Image ê°ì²´ ìƒì„±
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # ì–¼êµ´ ê°ì§€
        detection_result = self.face_detector.detect(mp_image)
        
        recognized_names = []
        h, w, _ = frame.shape
        
        if detection_result.detections:
            for detection in detection_result.detections:
                # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
                bbox = detection.bounding_box
                x_min = int(bbox.origin_x)
                y_min = int(bbox.origin_y)
                x_max = int(bbox.origin_x + bbox.width)
                y_max = int(bbox.origin_y + bbox.height)
                
                # ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œ
                current_features = self.extract_face_features(detection, w, h)
                
                # ë“±ë¡ëœ ì–¼êµ´ê³¼ ë¹„êµ
                best_match_name = "Unknown"
                best_similarity = 0
                
                for known_features, known_name in zip(self.known_face_features, self.known_face_names):
                    similarity = self.cosine_similarity(current_features, known_features)
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match_name = known_name
                
                # ìœ ì‚¬ë„ ì„ê³„ê°’ (ì„¤ì •ê°’ ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ)
                confidence = best_similarity * 100
                if (best_similarity * 100) < (self.face_similarity_threshold * 100):
                    best_match_name = "Unknown"
                    confidence = 0
                
                if best_match_name != "Unknown":
                    recognized_names.append((best_match_name, confidence))
                
                # ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                color = (0, 255, 0) if best_match_name != "Unknown" else (0, 0, 255)
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 2)
                
                # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ëˆˆ, ì½”, ì… ë“±)
                for keypoint in detection.keypoints:
                    kp_x = int(keypoint.x * w)
                    kp_y = int(keypoint.y * h)
                    cv2.circle(frame, (kp_x, kp_y), 2, (0, 255, 255), -1)
                
                # ì´ë¦„ê³¼ ì‹ ë¢°ë„ í‘œì‹œ
                label_height = 40
                cv2.rectangle(frame, (x_min, y_max), (x_max, y_max + label_height), color, cv2.FILLED)
                
                if best_match_name != "Unknown":
                    cv2.putText(frame, best_match_name, (x_min + 6, y_max + 15), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                    cv2.putText(frame, f"{confidence:.1f}%", (x_min + 6, y_max + 35), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255), 1)
                else:
                    cv2.putText(frame, "Unknown", (x_min + 6, y_max + 25), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
        
        return frame, recognized_names
    
    def recognize_faces_opencv(self, frame):
        """í´ë°±: OpenCV Haar Cascadeë¡œ ì–¼êµ´ ê°ì§€"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Haar Cascade ë¶ˆëŸ¬ì˜¤ê¸°
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        recognized_names = []
        
        for (x, y, w, h) in faces:
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(frame, "Face Detected", (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        return frame, recognized_names
    
    def recognize_gesture(self, frame, skip_cooldown=False):
        """í”„ë ˆì„ì—ì„œ ì œìŠ¤ì²˜ ì¸ì‹ (Quality Gate í¬í•¨)
        
        Args:
            frame: ì…ë ¥ í”„ë ˆì„
            skip_cooldown: Trueì¼ ê²½ìš° ì¿¨ë‹¤ìš´ ë¬´ì‹œ (ìë™ ì¸ì‹ ëª¨ë“œìš©)
        """
        if not MEDIAPIPE_AVAILABLE or not self.gesture_recognizer:
            return [], frame
        
        try:
            # MediaPipe Image ê°ì²´ ìƒì„±
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
            
            # ì œìŠ¤ì²˜ ì¸ì‹
            result = self.gesture_recognizer.recognize(mp_image)
            
            detected_gestures = []
            current_time = time.time()
            
            if hasattr(result, 'gestures') and result.gestures:
                for gesture_list in result.gestures:
                    if gesture_list:  # ì œìŠ¤ì²˜ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´
                        gesture = gesture_list[0]  # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ì œìŠ¤ì²˜
                        gesture_name = gesture.category_name
                        confidence = gesture.score
                        
                        # Quality Gate: ì‹ ë¢°ë„ ì„ê³„ê°’ í™•ì¸
                        if confidence < self.gesture_confidence_threshold:
                            continue
                        
                        # Cooldown í™•ì¸: ê°™ì€ ì œìŠ¤ì²˜ê°€ ìµœê·¼ì— ê°ì§€ë˜ì—ˆëŠ”ê°€?
                        # (ìë™ ì¸ì‹ ëª¨ë“œì—ì„œëŠ” cooldown ë¬´ì‹œ)
                        if not skip_cooldown:
                            if gesture_name in self.last_gesture_time:
                                if current_time - self.last_gesture_time[gesture_name] < self.gesture_cooldown:
                                    continue  # ì¿¨ë‹¤ìš´ ì¤‘ì´ë©´ ë¬´ì‹œ
                        
                        # ìœ íš¨í•œ ì œìŠ¤ì²˜: ì—…ë°ì´íŠ¸
                        self.last_gesture_time[gesture_name] = current_time
                        detected_gestures.append({
                            'type': gesture_name,
                            'confidence': confidence,
                            'timestamp': current_time
                        })
                        
                        print(f"âœ“ ì œìŠ¤ì²˜ ì¸ì‹: {gesture_name} ({confidence:.2f})")
            
            return detected_gestures, frame
            
        except Exception as e:
            print(f"âš ï¸  ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
            return [], frame
    
    def process_gesture_event(self, gesture_data):
        """ê°ì§€ëœ ì œìŠ¤ì²˜ ì²˜ë¦¬"""
        gesture_type = gesture_data['type']
        confidence = gesture_data['confidence']
        
        self.detected_gesture_label.setText(f"ì œìŠ¤ì²˜: {gesture_type} ({confidence:.1%})")
        
        # ZeroMQë¡œ ì„œë²„ì— ì „ì†¡ (í•„ìš”ì‹œ)
        print(f"ğŸ“¤ ì œìŠ¤ì²˜ ì´ë²¤íŠ¸: {gesture_type} ({confidence:.1%})")
    
    def update_frame(self):
        """ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
        ret, frame = self.camera.read()
        
        if ret:
            self.current_frame = frame
            display_frame = frame.copy()
            
            # ì–¼êµ´ ì¸ì‹ ëª¨ë“œ
            if self.current_mode == "face_attendance":
                display_frame, recognized_names = self.recognize_faces(display_frame)
                
                if recognized_names:
                    name, confidence = recognized_names[0]
                    self.user_name_label.setText(f"ì´ë¦„: {name}")
                    self.attendance_status_label.setText(f"ì¸ì‹ë¨ ({confidence:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    
                    if confidence > 80:
                        self.detected_gesture_label.setText(f"âœ“ {name} ì¶œì„ í™•ì¸")
                else:
                    self.detected_gesture_label.setText("ì–¼êµ´: ê°ì§€ ì•ˆë¨")
            
            # ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë“œ
            elif self.current_mode == "gesture_attendance":
                gestures, display_frame = self.recognize_gesture(display_frame)
                
                # ì œìŠ¤ì²˜ ì˜¤ë²„ë ˆì´ í‘œì‹œ
                overlay_y = 30
                if gestures:
                    for gesture in gestures:
                        self.process_gesture_event(gesture)
                        gesture_type = gesture['type']
                        confidence = gesture['confidence']
                        
                        gesture_text = f"Gesture: {gesture_type} ({confidence*100:.1f}%)"
                        cv2.putText(display_frame, gesture_text, (10, overlay_y), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        overlay_y += 30
                        
                        self.attendance_status_label.setText(f"ì œìŠ¤ì²˜ ê°ì§€ë¨!")
                        self.attendance_status_label.setStyleSheet(
                            f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                        )
                else:
                    cv2.putText(display_frame, "Gesture: Waiting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                    self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
                    self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
            # ìë™ ì¸ì‹ ëª¨ë“œ (ì–¼êµ´ + ì œìŠ¤ì²˜ + ìŒì„± ë™ì‹œ ì¸ì‹)
            elif self.current_mode == "automation":
                display_frame, face_results = self.recognize_faces(display_frame)
                # ìë™ ì¸ì‹ ëª¨ë“œì—ì„œëŠ” ì œìŠ¤ì²˜ ì¿¨ë‹¤ìš´ ë¬´ì‹œ
                gestures, display_frame = self.recognize_gesture(display_frame, skip_cooldown=True)
                
                # Decision Fusion Logic: ì–¼êµ´ + ì œìŠ¤ì²˜ ì¢…í•© íŒë‹¨
                face_score = 0
                face_name = "Unknown"
                
                if face_results:
                    face_name, face_confidence = face_results[0]
                    face_score = face_confidence / 100.0
                
                gesture_score = 0
                gesture_detected = None
                
                if gestures:
                    gesture_detected = gestures[0]['type']
                    gesture_score = gestures[0]['confidence']
                
                # í†µí•© ì ìˆ˜ ê³„ì‚° (ì–¼êµ´ 70%, ì œìŠ¤ì²˜ 30%)
                fusion_score = (face_score * 0.7) + (gesture_score * 0.3)
                
                # ë™ì‹œ ì˜¤ë²„ë ˆì´: ì¹´ë©”ë¼ í”„ë ˆì„ì— ì‹¤ì‹œê°„ í‘œì‹œ
                h, w, _ = display_frame.shape
                
                # 1. ì–¼êµ´ ì •ë³´ ì˜¤ë²„ë ˆì´
                overlay_y = 30
                if face_name != "Unknown":
                    face_text = f"Face: {face_name} ({face_score*100:.1f}%)"
                    cv2.putText(display_frame, face_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    overlay_y += 30
                else:
                    cv2.putText(display_frame, "Face: Detecting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
                    overlay_y += 30
                
                # 2. ì œìŠ¤ì²˜ ì •ë³´ ì˜¤ë²„ë ˆì´
                if gesture_detected:
                    gesture_text = f"Gesture: {gesture_detected} ({gesture_score*100:.1f}%)"
                    cv2.putText(display_frame, gesture_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    overlay_y += 30
                else:
                    cv2.putText(display_frame, "Gesture: Waiting...", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                    overlay_y += 30
                
                # 3. ìŒì„± ì •ë³´ ì˜¤ë²„ë ˆì´ (ìµœê·¼ 5ì´ˆ ì´ë‚´ ê²°ê³¼ í‘œì‹œ)
                current_time = time.time()
                voice_name = "Unknown"
                voice_score = 0.0
                
                if self.last_voice_result and (current_time - self.voice_result_time) < 5.0:
                    voice_name, voice_score = self.last_voice_result
                    voice_text = f"Voice: {voice_name} ({voice_score:.2f})"
                    cv2.putText(display_frame, voice_text, (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    cv2.putText(display_frame, "Voice: Press V key", (10, overlay_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)
                overlay_y += 30
                
                # 4. ì¢…í•© ì ìˆ˜ ì˜¤ë²„ë ˆì´ (ì–¼êµ´ 50%, ì œìŠ¤ì²˜ 25%, ìŒì„± 25%)
                fusion_score = (face_score * 0.5) + (gesture_score * 0.25) + (voice_score * 0.25)
                fusion_text = f"Fusion Score: {fusion_score*100:.1f}%"
                cv2.putText(display_frame, fusion_text, (10, overlay_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # ìë™ ì¸ì‹ ê¸°ì¤€: ì–¼êµ´ ì¸ì‹ë¥  > 70% ë˜ëŠ” (ì–¼êµ´ > 50% AND ì œìŠ¤ì²˜ ê°ì§€)
                if face_name != "Unknown" and face_score > 0.7:
                    # ì–¼êµ´ ì¸ì‹ ì„±ê³µ
                    self.user_name_label.setText(f"ì´ë¦„: {face_name}")
                    self.attendance_status_label.setText(f"âœ“ {face_name} ì¸ì‹ë¨ ({face_score*100:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    
                    status_msg = f"âœ“ {face_name} - ì–¼êµ´: {face_score*100:.1f}%"
                    if gesture_detected:
                        status_msg += f" + ì œìŠ¤ì²˜: {gesture_detected}"
                    self.detected_gesture_label.setText(status_msg)
                    
                elif face_name != "Unknown" and face_score > 0.5 and gesture_detected:
                    # ì–¼êµ´ + ì œìŠ¤ì²˜ ì¡°í•©ìœ¼ë¡œ ì¸ì‹
                    self.user_name_label.setText(f"ì´ë¦„: {face_name}")
                    self.attendance_status_label.setText(f"âœ“ ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹ ({fusion_score*100:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    self.detected_gesture_label.setText(f"ì–¼êµ´: {face_score*100:.1f}% + ì œìŠ¤ì²˜: {gesture_detected}")
                else:
                    # ëŒ€ê¸°
                    self.attendance_status_label.setText("ìë™ ì¸ì‹ ì¤‘...")
                    self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
                    
                    status_msg = ""
                    if face_name != "Unknown":
                        status_msg = f"ì–¼êµ´: {face_score*100:.1f}%"
                    if gesture_detected:
                        if status_msg:
                            status_msg += f" + ì œìŠ¤ì²˜: {gesture_detected}"
                        else:
                            status_msg = f"ì œìŠ¤ì²˜: {gesture_detected}"
                    
                    if status_msg:
                        self.detected_gesture_label.setText(status_msg)
                    else:
                        self.detected_gesture_label.setText("ëŒ€ê¸° ì¤‘...")
            
            # ëª¨ë“œ í‘œì‹œ
            if self.current_mode:
                mode_text = {
                    "gesture_attendance": "ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ",
                    "face_attendance": "ì–¼êµ´ ì¸ì‹ ëª¨ë“œ",
                    "automation": "ğŸ§  ìë™ ì¸ì‹ ëª¨ë“œ (ì–¼êµ´+ì œìŠ¤ì²˜+ìŒì„±)",
                }
                cv2.putText(display_frame, mode_text.get(self.current_mode, ""), 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # BGRì„ RGBë¡œ ë³€í™˜
            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            
            # QImageë¡œ ë³€í™˜
            h, w, ch = rgb_frame.shape
            bytes_per_line = ch * w
            qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
            
            # QLabelì— í‘œì‹œ
            pixmap = QPixmap.fromImage(qt_image)
            scaled_pixmap = pixmap.scaled(CAM_WIDTH, CAM_HEIGHT, Qt.KeepAspectRatio)
            self.camera_label.setPixmap(scaled_pixmap)
    
    def on_mode_button_click(self, mode_name):
        """ëª¨ë“œ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸"""
        self.current_mode = mode_name
        
        if mode_name == "gesture_attendance":
            self.update_status("âœ‹ ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "face_attendance":
            self.update_status("ğŸ˜Š ì–¼êµ´ ì¸ì‹ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì–¼êµ´ ì¸ì‹ ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "voice_attendance":
            self.update_status("ğŸ¤ ìŒì„± ì¸ì‹ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ìŒì„± íŒŒì¼ ì„ íƒ ëŒ€ê¸° ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
            # ìŒì„± íŒŒì¼ ì„ íƒ ë‹¤ì´ì–¼ë¡œê·¸ ì—´ê¸°
            audio_file, _ = QFileDialog.getOpenFileName(
                self,
                "ìŒì„± íŒŒì¼ ì„ íƒ",
                "",
                "ìŒì„± íŒŒì¼ (*.wav *.mp3 *.flac);;ëª¨ë“  íŒŒì¼ (*)"
            )
            
            if audio_file:
                self.update_status(f"ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘... ({os.path.basename(audio_file)})")
                self.attendance_status_label.setText("ì¸ì‹ ì¤‘...")
                
                # ìŒì„± ì¸ì‹ ì‹¤í–‰
                name, confidence = self.recognize_voice(audio_file)
                
                if name != "Unknown" and confidence > self.voice_similarity_threshold:
                    self.user_name_label.setText(f"ì´ë¦„: {name}")
                    self.attendance_status_label.setText(f"ìŒì„± ì¸ì‹ë¨ ({confidence:.2f})")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    self.update_status(f"âœ… {name} ìŒì„± ì¸ì‹ ì„±ê³µ")
                    self.process_voice_event(name, confidence)
                else:
                    self.attendance_status_label.setText("ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    self.update_status("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨ - ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
            else:
                self.current_mode = None
                self.update_status("âŒ ìŒì„± íŒŒì¼ ì„ íƒ ì·¨ì†Œë¨")
            
        elif mode_name == "automation":
            self.update_status("ğŸ§  ìë™ ì¸ì‹ ëª¨ë“œ í™œì„±í™” (ì–¼êµ´+ì œìŠ¤ì²˜+ìŒì„±)")
            self.attendance_status_label.setText("ìë™ ì¸ì‹ ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "attendance_status":
            self.update_status("ğŸ“Š ì¶œì„ í˜„í™© ì¡°íšŒ")
            QMessageBox.information(self, "ì¶œì„ í˜„í™©", "ì¶œì„ í˜„í™© ì¡°íšŒ ê¸°ëŠ¥ì…ë‹ˆë‹¤.")
    
    def update_status(self, message):
        """ìƒíƒœë°” ì—…ë°ì´íŠ¸"""
        self.status_bar.setText(message)
    
    def keyPressEvent(self, event):
        """í‚¤ë³´ë“œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
        from PyQt5.QtCore import Qt
        
        # ìë™í™” ëª¨ë“œì—ì„œ V í‚¤ë¥¼ ëˆ„ë¥´ë©´ ìŒì„± ì¸ì‹
        if self.current_mode == "automation" and event.key() == Qt.Key_V:
            self.trigger_voice_recognition()
        
        super().keyPressEvent(event)
    
    def trigger_voice_recognition(self):
        """ìŒì„± ì¸ì‹ íŠ¸ë¦¬ê±° (ìë™í™” ëª¨ë“œìš©)"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            self.update_status("âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        # ìŒì„± íŒŒì¼ ì„ íƒ
        audio_file, _ = QFileDialog.getOpenFileName(
            self,
            "ìŒì„± íŒŒì¼ ì„ íƒ",
            "",
            "ìŒì„± íŒŒì¼ (*.wav *.mp3 *.flac);;ëª¨ë“  íŒŒì¼ (*)"
        )
        
        if audio_file:
            self.update_status(f"ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘... ({os.path.basename(audio_file)})")
            
            # ìŒì„± ì¸ì‹ ì‹¤í–‰
            name, confidence = self.recognize_voice(audio_file)
            
            if name != "Unknown" and confidence > self.voice_similarity_threshold:
                # ìŒì„± ì¸ì‹ ê²°ê³¼ ì €ì¥ (5ì´ˆê°„ ìœ ì§€)
                self.last_voice_result = (name, confidence)
                self.voice_result_time = time.time()
                self.update_status(f"âœ… ìŒì„± ì¸ì‹ ì„±ê³µ: {name} ({confidence:.2f})")
            else:
                self.last_voice_result = None
                self.update_status("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
    
    def closeEvent(self, event):
        """ìœˆë„ìš° ì¢…ë£Œ ì´ë²¤íŠ¸"""
        if self.camera is not None:
            self.camera.release()
        if MEDIAPIPE_AVAILABLE:
            if hasattr(self, 'face_detector') and self.face_detector:
                self.face_detector.close()
            if hasattr(self, 'gesture_recognizer') and self.gesture_recognizer:
                self.gesture_recognizer.close()
        event.accept()


def main():
    app = QApplication(sys.argv)
    
    # ë‹¤í¬ í…Œë§ˆ ì ìš©
    app.setStyle('Fusion')
    palette = QPalette()
    palette.setColor(QPalette.Window, QColor(30, 39, 46))
    palette.setColor(QPalette.WindowText, Qt.white)
    app.setPalette(palette)
    
    window = ClientUI()
    window.show()
    
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()
