"""
í´ë¼ì´ì–¸íŠ¸ UI - ì¶œê²°ê´€ë¦¬ ì‹œìŠ¤í…œ
ì œìŠ¤ì²˜ ë° ì–¼êµ´ ì¸ì‹ ì¶œì„ ì²´í¬ (PyQt5 ê¸°ë°˜)
"""

import sys
import cv2
import os
import pickle
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QLabel, QPushButton,
    QVBoxLayout, QHBoxLayout, QMessageBox, QFrame
)
from PyQt5.QtCore import Qt, QTimer
from PyQt5.QtGui import QImage, QPixmap, QFont, QPalette, QColor

# MediaPipe import
MEDIAPIPE_AVAILABLE = False
USE_TASK_API = False
try:
    import mediapipe as mp
    # Task API import ì‹œë„
    try:
        from mediapipe.tasks import python
        from mediapipe.tasks.python import vision
        from mediapipe import Image as MPImage
        MEDIAPIPE_AVAILABLE = True
        USE_TASK_API = True
        print("âœ“ MediaPipe Task API ì‚¬ìš© ê°€ëŠ¥")
    except Exception:
        # Task API ì—†ìœ¼ë©´ OpenCVë§Œ ì‚¬ìš©
        USE_TASK_API = False
        MEDIAPIPE_AVAILABLE = False
        print("â„¹ï¸  MediaPipe Task APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenCVë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸  MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# SpeechBrain ìŒì„± ì¸ì‹ ëª¨ë¸ (ì˜µì…˜)
SPEECHBRAIN_AVAILABLE = False
try:
    import torchaudio
    from speechbrain.inference.speaker import EncoderClassifier
    SPEECHBRAIN_AVAILABLE = True
    print("âœ“ SpeechBrain ì‚¬ìš© ê°€ëŠ¥")
except Exception:
    SPEECHBRAIN_AVAILABLE = False
    print("â„¹ï¸  SpeechBrain(ìŒì„± ëª¨ë¸)ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# UI ì„¤ì • ì„í¬íŠ¸
from ui_config_lib import *


def put_korean_text(img, text, position, font_size=20, color=(255, 255, 255)):
    """PILì„ ì‚¬ìš©í•˜ì—¬ í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ë Œë”ë§."""
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)

    font = None
    # í›„ë³´ ê²½ë¡œë“¤
    candidates = []
    if os.name == 'nt':
        candidates += [r"C:\Windows\Fonts\malgun.ttf", r"C:\Windows\Fonts\gulim.ttc"]
    candidates += ["/usr/share/fonts/truetype/nanum/NanumGothic.ttf", "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"]
    candidates += [os.path.join(os.getcwd(), "fonts", "NanumGothic.ttf"), os.path.join(os.getcwd(), "fonts", "NotoSansCJK-Regular.ttc")]

    for path in candidates:
        try:
            if path and os.path.exists(path):
                font = ImageFont.truetype(path, font_size)
                break
        except Exception:
            continue

    if font is None:
        try:
            font = ImageFont.truetype("malgun.ttf", font_size)
        except Exception:
            try:
                font = ImageFont.truetype("NanumGothic.ttf", font_size)
            except Exception:
                font = ImageFont.load_default()

    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


class ClientUI(QMainWindow):
    """í´ë¼ì´ì–¸íŠ¸ ì¶œì„ ì²´í¬ ìœˆë„ìš°"""
    
    def __init__(self):
        super().__init__()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        self.camera = None
        self.current_frame = None
        self.current_mode = None  # 'gesture', 'face', None
        self.current_user = None
        
        # ì–¼êµ´ ê°ì§€ê¸° ì´ˆê¸°í™”
        self.face_detector = None
        self.gesture_recognizer = None
        
        if MEDIAPIPE_AVAILABLE and USE_TASK_API:
            try:
                # ì–¼êµ´ ê°ì§€ê¸°
                base_options_face = python.BaseOptions(model_asset_path='../models/blaze_face_short_range.tflite')
                face_options = vision.FaceDetectorOptions(base_options=base_options_face)
                self.face_detector = vision.FaceDetector.create_from_options(face_options)
                
                # ì œìŠ¤ì²˜ ì¸ì‹ê¸°
                base_options_gesture = python.BaseOptions(model_asset_path='../models/gesture_recognizer.task')
                gesture_options = vision.GestureRecognizerOptions(base_options=base_options_gesture)
                self.gesture_recognizer = vision.GestureRecognizer.create_from_options(gesture_options)
                
                print("âœ“ MediaPipe Task API ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("â„¹ï¸  ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”: models/blaze_face_short_range.tflite, models/gesture_recognizer.task")
        
        # ì–¼êµ´ì¸ì‹ ë°ì´í„° ë¡œë“œ (name -> samples ë”•ì…”ë„ˆë¦¬)
        self.known_face_db = {}
        self.load_face_data()

        # ìŒì„± ì¸ì‹ ì¤€ë¹„ (ì˜µì…˜)
        self.voice_encoder = None
        self.known_voice_embeddings = []
        self.known_voice_names = []
        if SPEECHBRAIN_AVAILABLE:
            try:
                # ëª¨ë¸ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤
                self.voice_encoder = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir="../models/spkrec-ecapa-voxceleb"
                )
                self.load_voice_data()
                print("âœ“ ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # UI ì´ˆê¸°í™”
        self.init_ui()

        # ì¹´ë©”ë¼ ì‹œì‘
        self.start_camera()
        
    def init_ui(self):
        """UI ì´ˆê¸°í™”"""
        # ìœˆë„ìš° ì„¤ì •
        self.setWindowTitle(WINDOW_TITLE)
        self.setGeometry(100, 100, WINDOW_WIDTH, WINDOW_HEIGHT)
        self.setStyleSheet(f"background-color: {BG_COLOR};")
        
        # ì¤‘ì•™ ìœ„ì ¯
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ (ìˆ˜í‰)
        main_layout = QHBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±
        sidebar = self.create_sidebar()
        main_layout.addWidget(sidebar)
        
        # ì¤‘ì•™ + ìš°ì¸¡ ì˜ì—­
        center_right_layout = QVBoxLayout()
        center_right_layout.setContentsMargins(20, 20, 20, 20)
        
        # ì¤‘ì•™(ì¹´ë©”ë¼) + ìš°ì¸¡(ì •ë³´) ì˜ì—­
        cam_info_layout = QHBoxLayout()
        
        # ì¹´ë©”ë¼ ì˜ì—­
        self.camera_label = self.create_camera_view()
        cam_info_layout.addWidget(self.camera_label)
        
        # ìš°ì¸¡ ì •ë³´ íŒ¨ë„
        right_panel = self.create_right_panel()
        cam_info_layout.addWidget(right_panel)
        
        center_right_layout.addLayout(cam_info_layout)
        
        # í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ
        gesture_guide = self.create_gesture_guide()
        center_right_layout.addWidget(gesture_guide)
        
        # í•˜ë‹¨ ìƒíƒœë°”
        status_bar = self.create_status_bar()
        center_right_layout.addWidget(status_bar)
        
        main_layout.addLayout(center_right_layout)
        
    def create_sidebar(self):
        """ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìƒì„±"""
        sidebar = QWidget()
        sidebar.setFixedWidth(SIDEBAR_WIDTH)
        sidebar.setStyleSheet(f"background-color: {SIDEBAR_COLOR};")
        
        layout = QVBoxLayout(sidebar)
        layout.setContentsMargins(SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING, SIDEBAR_PADDING)
        layout.setSpacing(0)
        
        # í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ ë¼ë²¨
        client_label = QLabel("ğŸ“± ì¶œì„ ì²´í¬")
        client_label.setFixedHeight(CLIENT_LABEL_HEIGHT)
        client_label.setAlignment(Qt.AlignCenter)
        client_label.setStyleSheet(f"""
            color: {TEXT_COLOR};
            font-size: {CLIENT_LABEL_FONT_SIZE}px;
            font-weight: {CLIENT_LABEL_FONT_WEIGHT};
            background-color: {ACCENT_COLOR};
            border-radius: 5px;
            padding: 10px;
        """)
        layout.addWidget(client_label)
        
        layout.addSpacing(LEFT_BUTTON_START_Y - CLIENT_LABEL_HEIGHT - SIDEBAR_PADDING)
        
        # ì¢Œì¸¡ ë²„íŠ¼ë“¤ ìƒì„±
        self.left_buttons = {}
        for btn_config in LEFT_BUTTONS:
            btn = QPushButton(btn_config["text"])
            btn.setFixedSize(LEFT_BUTTON_WIDTH, LEFT_BUTTON_HEIGHT)
            btn.setStyleSheet(self.get_button_style())
            btn.setCursor(Qt.PointingHandCursor)
            
            # ë²„íŠ¼ ì´ë²¤íŠ¸ ì—°ê²°
            btn_name = btn_config["name"]
            btn.clicked.connect(lambda checked, name=btn_name: self.on_mode_button_click(name))
            
            self.left_buttons[btn_name] = btn
            layout.addWidget(btn)
            layout.addSpacing(LEFT_BUTTON_SPACING)
        
        layout.addStretch()
        
        return sidebar
    
    def create_camera_view(self):
        """ì¹´ë©”ë¼ ë·° ìƒì„±"""
        camera_label = QLabel()
        camera_label.setFixedSize(CAM_WIDTH, CAM_HEIGHT)
        camera_label.setAlignment(Qt.AlignCenter)
        camera_label.setStyleSheet(f"""
            background-color: {CAM_BG_COLOR};
            border: 3px solid {ACCENT_COLOR};
            border-radius: 10px;
        """)
        camera_label.setText("ğŸ“¹ ì¹´ë©”ë¼ ë¡œë”© ì¤‘...")
        camera_label.setFont(QFont("Arial", 14))
        camera_label.setStyleSheet(camera_label.styleSheet() + f"color: {TEXT_COLOR};")
        
        return camera_label
    
    def create_right_panel(self):
        """ìš°ì¸¡ ì •ë³´ íŒ¨ë„ ìƒì„±"""
        panel = QWidget()
        panel.setFixedWidth(RIGHT_PANEL_WIDTH)
        
        layout = QVBoxLayout(panel)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(15)
        
        # ì‚¬ìš©ì ì •ë³´ í”„ë ˆì„
        user_info_frame = QFrame()
        user_info_frame.setFixedHeight(USER_INFO_HEIGHT)
        user_info_frame.setStyleSheet(f"""
            background-color: {USER_INFO_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        user_info_layout = QVBoxLayout(user_info_frame)
        
        user_title = QLabel("ğŸ‘¤ ì‚¬ìš©ì ì •ë³´")
        user_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        user_info_layout.addWidget(user_title)
        
        self.user_name_label = QLabel("ì´ë¦„: -")
        self.user_name_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_name_label)
        
        self.user_id_label = QLabel("í•™ë²ˆ: -")
        self.user_id_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        user_info_layout.addWidget(self.user_id_label)
        
        user_info_layout.addStretch()
        
        layout.addWidget(user_info_frame)
        
        # ì¶œì„ ìƒíƒœ í”„ë ˆì„
        status_frame = QFrame()
        status_frame.setFixedHeight(ATTENDANCE_STATUS_HEIGHT)
        status_frame.setStyleSheet(f"""
            background-color: {ATTENDANCE_STATUS_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        status_layout = QVBoxLayout(status_frame)
        
        status_title = QLabel("ğŸ“Š ì¶œì„ ìƒíƒœ")
        status_title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 14px; font-weight: bold;")
        status_layout.addWidget(status_title)
        
        self.attendance_status_label = QLabel("ëŒ€ê¸° ì¤‘...")
        self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
        self.attendance_status_label.setAlignment(Qt.AlignCenter)
        status_layout.addWidget(self.attendance_status_label)
        
        self.detected_gesture_label = QLabel("ì œìŠ¤ì²˜: -")
        self.detected_gesture_label.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 12px;")
        status_layout.addWidget(self.detected_gesture_label)
        
        status_layout.addStretch()
        
        layout.addWidget(status_frame)
        
        layout.addStretch()
        
        return panel
    
    def create_gesture_guide(self):
        """í•˜ë‹¨ ì œìŠ¤ì²˜ ê°€ì´ë“œ ìƒì„±"""
        guide_frame = QFrame()
        guide_frame.setFixedHeight(GESTURE_GUIDE_HEIGHT)
        guide_frame.setStyleSheet(f"""
            background-color: {GESTURE_GUIDE_BG_COLOR};
            border-radius: 8px;
            padding: 10px;
        """)
        
        layout = QVBoxLayout(guide_frame)
        
        title = QLabel("ğŸ‘‹ ì œìŠ¤ì²˜ ê°€ì´ë“œ")
        title.setStyleSheet(f"color: {TEXT_COLOR}; font-size: 13px; font-weight: bold;")
        layout.addWidget(title)
        
        # ì œìŠ¤ì²˜ ëª©ë¡
        gestures_layout = QHBoxLayout()
        
        for gesture_name, gesture_info in GESTURES.items():
            gesture_widget = QLabel(f"{gesture_info['emoji']}\n{gesture_info['text']}")
            gesture_widget.setAlignment(Qt.AlignCenter)
            gesture_widget.setStyleSheet(f"""
                color: {gesture_info['color']};
                font-size: 11px;
                padding: 5px;
            """)
            gestures_layout.addWidget(gesture_widget)
        
        layout.addLayout(gestures_layout)
        
        return guide_frame
    
    def create_status_bar(self):
        """í•˜ë‹¨ ìƒíƒœë°” ìƒì„±"""
        status_bar = QLabel("âœ… ì¤€ë¹„ ì™„ë£Œ - ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
        status_bar.setFixedHeight(STATUS_BAR_HEIGHT)
        status_bar.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)
        status_bar.setStyleSheet(f"""
            background-color: {STATUS_BAR_BG_COLOR};
            color: {TEXT_COLOR};
            font-size: {STATUS_FONT_SIZE}px;
            padding-left: 15px;
            border-radius: 5px;
        """)
        self.status_bar = status_bar
        return status_bar
    
    def get_button_style(self, font_size=LEFT_BUTTON_FONT_SIZE):
        """ë²„íŠ¼ ìŠ¤íƒ€ì¼ ë°˜í™˜"""
        return f"""
            QPushButton {{
                background-color: {BUTTON_COLOR};
                color: {TEXT_COLOR};
                border: none;
                border-radius: 5px;
                font-size: {font_size}px;
                font-weight: bold;
                padding: 5px;
            }}
            QPushButton:hover {{
                background-color: {BUTTON_HOVER_COLOR};
            }}
            QPushButton:pressed {{
                background-color: #1B1464;
            }}
        """
    
    def start_camera(self):
        """ì¹´ë©”ë¼ ì‹œì‘"""
        self.camera = cv2.VideoCapture(CAMERA_INDEX)
        
        if not self.camera.isOpened():
            self.update_status("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì¹´ë©”ë¼ í•´ìƒë„ ì„¤ì •
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, CAM_WIDTH)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, CAM_HEIGHT)
        
        # íƒ€ì´ë¨¸ë¡œ í”„ë ˆì„ ì—…ë°ì´íŠ¸
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(int(1000 / CAMERA_FPS))
        
        self.update_status("ğŸ“¹ ì¹´ë©”ë¼ í™œì„±í™”ë¨")
    
    def load_face_data(self):
        """ì €ì¥ëœ ì–¼êµ´ ë°ì´í„° ë¡œë“œ (face_db dict expected)"""
        face_data_file = "../data/face_data.pkl"
        self.known_face_db = {}

        if os.path.exists(face_data_file):
            try:
                with open(face_data_file, 'rb') as f:
                    data = pickle.load(f)
                    if 'face_db' in data:
                        self.known_face_db = data.get('face_db', {})
                    else:
                        # legacy format
                        features = data.get('features', [])
                        names = data.get('names', [])
                        for feat, name in zip(features, names):
                            self.known_face_db.setdefault(name, []).append(feat)
                total_people = len(self.known_face_db)
                total_samples = sum(len(v) for v in self.known_face_db.values())
                print(f"âœ“ {total_people}ëª…ì˜ ì–¼êµ´ ë°ì´í„° ë¡œë“œë¨, ì´ ìƒ˜í”Œ {total_samples}ê°œ")
            except Exception as e:
                print(f"âš ï¸  ì–¼êµ´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("â„¹ï¸  ë“±ë¡ëœ ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def load_voice_data(self):
        """ì €ì¥ëœ ìŒì„±(ì„ë² ë”©) ë°ì´í„° ë¡œë“œ"""
        voice_data_file = "../data/voice_data.pkl"

        if os.path.exists(voice_data_file):
            try:
                with open(voice_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_voice_embeddings = data.get('embeddings', [])
                    self.known_voice_names = data.get('names', [])
                print(f"âœ“ {len(self.known_voice_names)}ëª…ì˜ ìŒì„± ë°ì´í„° ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ìŒì„± ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print("â„¹ï¸  ë“±ë¡ëœ ìŒì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def save_voice_data(self):
        """ìŒì„± ì„ë² ë”© ì €ì¥"""
        voice_data_file = "../data/voice_data.pkl"
        os.makedirs(os.path.dirname(voice_data_file), exist_ok=True)

        data = {
            'embeddings': self.known_voice_embeddings,
            'names': self.known_voice_names
        }

        try:
            with open(voice_data_file, 'wb') as f:
                pickle.dump(data, f)
            print("âœ“ ìŒì„± ë°ì´í„° ì €ì¥ë¨")
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def extract_face_features(self, detection, image_width, image_height):
        """ì–¼êµ´ ê°ì§€ ê²°ê³¼ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        bbox = detection.bounding_box
        features = [
            bbox.origin_x / image_width,
            bbox.origin_y / image_height,
            bbox.width / image_width,
            bbox.height / image_height
        ]
        return np.array(features)
    
    def cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    def recognize_faces(self, frame):
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ì¸ì‹ (MediaPipe Task API ê¸°ë°˜)"""
        if not self.face_detector:
            # MediaPipeê°€ ì—†ìœ¼ë©´ OpenCV Haar Cascade ì‚¬ìš©
            return self.recognize_faces_opencv(frame)
        
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb_frame = np.ascontiguousarray(rgb_frame, dtype=np.uint8)

        detection_result = None
        try:
            mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
            detection_result = self.face_detector.detect(mp_image)
        except Exception as e:
            try:
                mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb_frame.tobytes())
                detection_result = self.face_detector.detect(mp_image)
            except Exception as e2:
                print(f"âš ï¸ MediaPipe detection error (skipping this frame): {e2}")
                detection_result = None
        
        recognized_names = []
        h, w, _ = frame.shape
        
        if detection_result.detections:
            for detection in detection_result.detections:
                bbox = detection.bounding_box
                x_min = int(bbox.origin_x)
                y_min = int(bbox.origin_y)
                x_max = int(bbox.origin_x + bbox.width)
                y_max = int(bbox.origin_y + bbox.height)
                
                current_features = self.extract_face_features(detection, w, h)
                
                best_match_name = "Unknown"
                best_similarity = 0

                for known_name, samples in self.known_face_db.items():
                    max_sim = 0
                    for known_features in samples:
                        sim = self.cosine_similarity(current_features, np.array(known_features))
                        if sim > max_sim:
                            max_sim = sim
                    if max_sim > best_similarity:
                        best_similarity = max_sim
                        best_match_name = known_name

                confidence = best_similarity * 100
                if confidence < 98:
                    best_match_name = "Unknown"
                    confidence = 0

                if best_match_name != "Unknown":
                    recognized_names.append((best_match_name, confidence))
                
                color = (0, 255, 0) if best_match_name != "Unknown" else (0, 0, 255)
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 2)
                
                for keypoint in detection.keypoints:
                    kp_x = int(keypoint.x * w)
                    kp_y = int(keypoint.y * h)
                    cv2.circle(frame, (kp_x, kp_y), 2, (0, 255, 255), -1)
                
                label_height = 40
                cv2.rectangle(frame, (x_min, y_max), (x_max, y_max + label_height), color, cv2.FILLED)
                
                if best_match_name != "Unknown":
                    cv2.putText(frame, best_match_name, (x_min + 6, y_max + 15), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                    cv2.putText(frame, f"{confidence:.1f}%", (x_min + 6, y_max + 35), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255), 1)
                else:
                    cv2.putText(frame, "Unknown", (x_min + 6, y_max + 25), 
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
        
        return frame, recognized_names
    
    def recognize_faces_opencv(self, frame):
        """í´ë°±: OpenCV Haar Cascadeë¡œ ì–¼êµ´ ê°ì§€"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        recognized_names = []
        
        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            frame = put_korean_text(frame, "ì–¼êµ´ ê°ì§€ë¨", (x, y-10), 18, (0, 255, 0))
        
        return frame, recognized_names

    def recognize_gesture(self, frame):
        """ì œìŠ¤ì²˜ ì¸ì‹ (MediaPipe Task API ê¸°ë°˜ ë˜ëŠ” í´ë°±)"""
        if MEDIAPIPE_AVAILABLE and hasattr(self, 'gesture_recognizer') and self.gesture_recognizer:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_image = MPImage(image_format=mp.ImageFormat.SRGB, data=rgb)
            try:
                result = self.gesture_recognizer.recognize(mp_image)
                gestures = []
                if hasattr(result, 'gestures') and result.gestures:
                    for g in result.gestures:
                        gestures.append(g[0].category_name)
                return gestures
            except Exception as e:
                print(f"âš ï¸  ì œìŠ¤ì²˜ ì¸ì‹ ì‹¤íŒ¨: {e}")
                return []
        else:
            return []

    def record_voice_and_extract(self, filename="./tmp_voice.wav", duration=3, fs=16000):
        """ìŒì„± ë…¹ìŒ(ê°„ë‹¨í•œ placeholder)"""
        if not SPEECHBRAIN_AVAILABLE or self.voice_encoder is None:
            print("â„¹ï¸  SpeechBrainì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        try:
            signal, sr = torchaudio.load(filename)
            emb = self.voice_encoder.encode_batch(signal)
            return emb.detach().cpu().numpy()
        except Exception as e:
            print(f"âš ï¸  ìŒì„± ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def update_frame(self):
        """ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
        ret, frame = self.camera.read()
        
        if ret:
            self.current_frame = frame
            display_frame = frame.copy()
            
            if self.current_mode == "face_attendance":
                display_frame, recognized_names = self.recognize_faces(display_frame)
                
                if recognized_names:
                    name, confidence = recognized_names[0]  # ì²« ë²ˆì§¸ ì¸ì‹ëœ ì‚¬ëŒ
                    self.user_name_label.setText(f"ì´ë¦„: {name}")
                    self.attendance_status_label.setText(f"ì¸ì‹ë¨ ({confidence:.1f}%)")
                    self.attendance_status_label.setStyleSheet(
                        f"color: {SUCCESS_COLOR}; font-size: 16px; font-weight: bold;"
                    )
                    
                    # ìë™ ì¶œì„ ì²˜ë¦¬ (confidence > 80%)
                    if confidence > 80:
                        # ì—¬ê¸°ì— ì¶œì„ ê¸°ë¡ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
                        pass
            elif self.current_mode == "gesture_attendance":
                gestures = self.recognize_gesture(display_frame)
                if gestures:
                    gtext = gestures[0]
                    display_frame = put_korean_text(display_frame, f"ì œìŠ¤ì²˜: {gtext}", (10, 30), 20, (255, 255, 255))
                    self.attendance_status_label.setText(f"ì œìŠ¤ì²˜ ì¸ì‹: {gtext}")
                else:
                    self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
            elif self.current_mode == "voice_attendance":
                self.attendance_status_label.setText("ìŒì„± ì…ë ¥ ëŒ€ê¸° (íŒŒì¼ ê¸°ë°˜)")
            
            if self.current_mode:
                mode_text = {
                    "gesture_attendance": "ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ",
                    "face_attendance": "ì–¼êµ´ ì¸ì‹ ëª¨ë“œ",
                }
                cv2.putText(display_frame, mode_text.get(self.current_mode, ""), 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            rgb_for_qt = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            rgb_for_qt = np.ascontiguousarray(rgb_for_qt, dtype=np.uint8)
            h, w, ch = rgb_for_qt.shape
            bytes_per_line = ch * w
            try:
                qt_image = QImage(rgb_for_qt.data, w, h, bytes_per_line, QImage.Format_RGB888)
                if qt_image.isNull():
                    raise ValueError("QImage is null")
            except Exception as e:
                print(f"âš ï¸ QImage creation failed, downscaling for stability: {e}")
                small = cv2.resize(rgb_for_qt, (min(w, 320), int(h * (min(w, 320) / w))))
                small = np.ascontiguousarray(small, dtype=np.uint8)
                h2, w2, ch2 = small.shape
                bytes_per_line2 = ch2 * w2
                qt_image = QImage(small.data, w2, h2, bytes_per_line2, QImage.Format_RGB888)

            try:
                pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(CAM_WIDTH, CAM_HEIGHT, Qt.KeepAspectRatio)
                self.camera_label.setPixmap(scaled_pixmap)
            except Exception as e:
                print(f"âš ï¸ Pixmap update failed: {e}")
    
    def on_mode_button_click(self, mode_name):
        """ëª¨ë“œ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸"""
        self.current_mode = mode_name
        
        if mode_name == "gesture_attendance":
            self.update_status("âœ‹ ì œìŠ¤ì²˜ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì œìŠ¤ì²˜ ëŒ€ê¸° ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {WARNING_COLOR}; font-size: 16px; font-weight: bold;")
            
        elif mode_name == "face_attendance":
            self.update_status("ğŸ˜Š ì–¼êµ´ ì¸ì‹ ì¶œì„ ëª¨ë“œ í™œì„±í™”")
            self.attendance_status_label.setText("ì–¼êµ´ ì¸ì‹ ì¤‘...")
            self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")

        elif mode_name == "voice_attendance":
            self.update_status("ğŸ“¢ ìŒì„± ì¸ì‹ ëª¨ë“œ í™œì„±í™”")
            if SPEECHBRAIN_AVAILABLE and self.voice_encoder:
                self.attendance_status_label.setText("ìŒì„± ì…ë ¥ ëŒ€ê¸° (íŒŒì¼ ê¸°ë°˜)")
                self.attendance_status_label.setStyleSheet(f"color: {ACCENT_COLOR}; font-size: 16px; font-weight: bold;")
                QMessageBox.information(self, "ìŒì„± ì¸ì‹", "ë…¹ìŒëœ WAV íŒŒì¼ì„ ì¤€ë¹„í•œ ë’¤ 'ìŒì„± ì¸ì‹' ë²„íŠ¼(ì„ì‹œ)ì„ ëˆ„ë¥´ì„¸ìš”.")
            else:
                QMessageBox.warning(self, "ìŒì„± ì¸ì‹", "SpeechBrainì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
        elif mode_name == "attendance_status":
            self.update_status("ğŸ“Š ì¶œì„ í˜„í™© ì¡°íšŒ")
            QMessageBox.information(self, "ì¶œì„ í˜„í™©", "ì¶œì„ í˜„í™© ì¡°íšŒ ê¸°ëŠ¥ì…ë‹ˆë‹¤.")
    
    def update_status(self, message):
        """ìƒíƒœë°” ì—…ë°ì´íŠ¸"""
        self.status_bar.setText(message)
    
    def closeEvent(self, event):
        """ìœˆë„ìš° ì¢…ë£Œ ì´ë²¤íŠ¸"""
        if self.camera is not None:
            self.camera.release()
        if MEDIAPIPE_AVAILABLE:
            if hasattr(self, 'face_detector') and self.face_detector:
                self.face_detector.close()
            if hasattr(self, 'gesture_recognizer') and self.gesture_recognizer:
                self.gesture_recognizer.close()
        event.accept()


def main():
    app = QApplication(sys.argv)
    
    # ë‹¤í¬ í…Œë§ˆ ì ìš©
    app.setStyle('Fusion')
    palette = QPalette()
    palette.setColor(QPalette.Window, QColor(30, 39, 46))
    palette.setColor(QPalette.WindowText, Qt.white)
    app.setPalette(palette)
    
    window = ClientUI()
    window.show()
    
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()
